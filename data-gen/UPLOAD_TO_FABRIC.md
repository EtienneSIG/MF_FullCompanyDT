# Upload donn√©es vers Microsoft Fabric Lakehouse

## Contexte
Les fichiers CSV ont √©t√© g√©n√©r√©s localement dans `output/structured/` et doivent √™tre upload√©s vers le Lakehouse Fabric pour √™tre ing√©r√©s dans la couche Bronze.

## Structure attendue dans Fabric

Dans votre Lakehouse Fabric, cr√©ez cette structure dans **Files** :

```
üìÅ Files
  ‚îî‚îÄ üìÅ bronze
      ‚îú‚îÄ üìÅ dimensions
      ‚îÇ   ‚îú‚îÄ DimDate.csv
      ‚îÇ   ‚îú‚îÄ DimCustomer.csv
      ‚îÇ   ‚îú‚îÄ DimProduct.csv
      ‚îÇ   ‚îú‚îÄ DimEmployee.csv
      ‚îÇ   ‚îú‚îÄ DimGeography.csv
      ‚îÇ   ‚îú‚îÄ DimFacility.csv
      ‚îÇ   ‚îú‚îÄ DimProject.csv
      ‚îÇ   ‚îî‚îÄ DimAccount.csv
      ‚îú‚îÄ üìÅ crm
      ‚îÇ   ‚îú‚îÄ FactOpportunities.csv
      ‚îÇ   ‚îî‚îÄ FactActivities.csv
      ‚îú‚îÄ üìÅ sales
      ‚îÇ   ‚îú‚îÄ FactSales.csv
      ‚îÇ   ‚îî‚îÄ FactReturns.csv
      ‚îú‚îÄ üìÅ hr
      ‚îÇ   ‚îú‚îÄ FactAttrition.csv
      ‚îÇ   ‚îî‚îÄ FactHiring.csv
      ‚îú‚îÄ üìÅ supply_chain
      ‚îÇ   ‚îú‚îÄ FactInventory.csv
      ‚îÇ   ‚îî‚îÄ FactPurchaseOrders.csv
      ‚îú‚îÄ üìÅ manufacturing
      ‚îÇ   ‚îú‚îÄ FactProduction.csv
      ‚îÇ   ‚îî‚îÄ FactWorkOrders.csv
      ‚îú‚îÄ üìÅ finance
      ‚îÇ   ‚îú‚îÄ FactGeneralLedger.csv
      ‚îÇ   ‚îî‚îÄ FactBudget.csv
      ‚îú‚îÄ üìÅ esg
      ‚îÇ   ‚îî‚îÄ FactEmissions.csv
      ‚îú‚îÄ üìÅ call_center
      ‚îÇ   ‚îî‚îÄ FactSupport.csv
      ‚îú‚îÄ üìÅ itops
      ‚îÇ   ‚îî‚îÄ FactIncidents.csv
      ‚îú‚îÄ üìÅ finops
      ‚îÇ   ‚îî‚îÄ FactCloudCosts.csv
      ‚îú‚îÄ üìÅ rd
      ‚îÇ   ‚îî‚îÄ FactExperiments.csv
      ‚îú‚îÄ üìÅ quality_security
      ‚îÇ   ‚îú‚îÄ FactDefects.csv
      ‚îÇ   ‚îî‚îÄ FactSecurityEvents.csv
      ‚îú‚îÄ üìÅ risk_compliance
      ‚îÇ   ‚îú‚îÄ FactRisks.csv
      ‚îÇ   ‚îú‚îÄ FactAudits.csv
      ‚îÇ   ‚îî‚îÄ FactComplianceChecks.csv
      ‚îú‚îÄ üìÅ marketing
      ‚îÇ   ‚îî‚îÄ FactCampaigns.csv
      ‚îî‚îÄ üìÅ product
          ‚îî‚îÄ DimProductBOM.csv
```

## M√©thodes d'upload

### Option 1 : Via l'interface Fabric (recommand√© pour les petits datasets)

1. Ouvrez votre Lakehouse dans Microsoft Fabric
2. Allez dans **Files** > **bronze**
3. Cr√©ez les sous-dossiers si n√©cessaire
4. Uploadez les fichiers CSV depuis `output/structured/` vers les dossiers correspondants

### Option 2 : Via OneLake File Explorer (recommand√© pour les gros datasets)

1. Installez **OneLake File Explorer** si ce n'est pas d√©j√† fait
2. Montez votre Lakehouse comme lecteur r√©seau
3. Copiez-collez tous les dossiers de `output/structured/` vers `[OneLake Drive]/YourLakehouse/Files/bronze/`

### Option 3 : Via script PowerShell (si OneLake est mont√©)

```powershell
# Exemple si OneLake est mont√© en tant que lecteur O:
$source = ".\output\structured\*"
$destination = "O:\YourWorkspace\YourLakehouse\Files\bronze\"

Copy-Item -Path $source -Destination $destination -Recurse -Force
```

### Option 4 : Via Azure Data Factory / Pipeline Fabric

Cr√©ez un pipeline pour copier les fichiers depuis Azure Blob Storage ou autre source vers le Lakehouse.

## V√©rification

Apr√®s l'upload, v√©rifiez dans Fabric que vous avez :
- ‚úÖ 8 fichiers dans `bronze/dimensions/`
- ‚úÖ 2 fichiers dans `bronze/supply_chain/`
- ‚úÖ 2 fichiers dans `bronze/manufacturing/`
- ‚úÖ etc.

Une fois les fichiers upload√©s, relancez le notebook **01_ingest_to_bronze** !

## Notes importantes

‚ö†Ô∏è **Noms de dossiers** : Utilisez les noms avec underscores (`supply_chain`, `call_center`, etc.)  
‚ö†Ô∏è **Taille** : Le fichier `FactInventory.csv` fait ~912K lignes et peut √™tre volumineux  
‚ö†Ô∏è **Encodage** : Les fichiers sont en UTF-8
