# Workflow complet - De la gÃ©nÃ©ration de donnÃ©es Ã  l'analyse

## Ã‰tape 1 : GÃ©nÃ©rer les donnÃ©es (LOCAL) âœ… FAIT

```powershell
cd MF_FullCompanyDT\data-gen
python generate_all.py
```

**RÃ©sultat** : Fichiers CSV dans `output/structured/`
- 1,622,192 enregistrements
- 16 domaines (crm, sales, supply_chain, etc.)

---

## Ã‰tape 2 : Uploader vers Fabric Lakehouse âš ï¸ Ã€ FAIRE

### Option A : Via OneLake File Explorer (RecommandÃ©)

1. Installez **OneLake File Explorer** si nÃ©cessaire
2. Montez votre Lakehouse
3. Copiez le contenu de `output/structured/` vers :
   ```
   [OneLake Drive]\[Workspace]\[Lakehouse]\Files\bronze\
   ```

### Option B : Script automatique (si OneLake montÃ©)

```python
# Dans upload_to_fabric.py, modifiez :
ONELAKE_PATH = r"O:\VotreWorkspace\VotreLakehouse\Files\bronze"

# Puis exÃ©cutez :
python upload_to_fabric.py
```

### Option C : Via l'interface Fabric (Pour petits volumes)

1. Ouvrez votre Lakehouse dans Fabric
2. Cliquez sur **Files** > **Upload** > **Upload folder**
3. Uploadez chaque dossier de domaine depuis `output/structured/`

**VÃ©rification** : Dans Fabric, vous devriez voir :
```
ğŸ“ Files
  â””â”€ ğŸ“ bronze
      â”œâ”€ ğŸ“ dimensions (8 fichiers CSV)
      â”œâ”€ ğŸ“ crm (2 fichiers)
      â”œâ”€ ğŸ“ sales (2 fichiers)
      â”œâ”€ ğŸ“ supply_chain (2 fichiers)
      â””â”€ ... (etc.)
```

---

## Ã‰tape 3 : ExÃ©cuter 01_ingest_to_bronze.ipynb (FABRIC) âš ï¸ Ã€ FAIRE

**Objectif** : IngÃ©rer les CSV en tables Delta Bronze

1. Ouvrez le notebook dans Fabric
2. Attachez votre Lakehouse
3. ExÃ©cutez toutes les cellules (Run All)

**RÃ©sultat attendu** :
```
âœ… Dimensions ingested: 8/8
âœ… Fact tables ingested: 23
â­ï¸  Skipped: 0
âŒ Failed: 0
```

**Tables crÃ©Ã©es** :
- DimDate, DimCustomer, DimProduct, DimEmployee, etc.
- FactSales, FactInventory, FactSupport, etc.

---

## Ã‰tape 4 : ExÃ©cuter 02_transform_to_silver.ipynb (FABRIC) â³ VOUS ÃŠTES ICI

**Objectif** : Transformer Bronze â†’ Silver avec data quality

1. VÃ©rifiez que le notebook 01 a rÃ©ussi
2. ExÃ©cutez le notebook 02

**RÃ©sultat attendu** :
```
STEP 1: Transforming Dimension Tables
âœ… Silver_DimCustomer created: 50,000 â†’ 50,000 rows

STEP 2: Transforming Fact Tables
âœ… Silver_FactSales created: 201,221 â†’ 201,221 rows

STEP 3: Data Quality Validation
âœ… silver_dimcustomer: 0 nulls in customer_id
âœ… Primary key uniqueness: OK
```

**Erreur actuelle** : Les tables Bronze n'existent pas encore, donc rien Ã  transformer.

---

## Ã‰tape 5 : ExÃ©cuter 03_build_gold_star_schema.ipynb (FABRIC)

**Objectif** : CrÃ©er le schÃ©ma en Ã©toile optimisÃ© pour l'analyse

1. Fusionne les dimensions et faits
2. CrÃ©e les tables Gold dÃ©normalisÃ©es
3. Optimise pour les requÃªtes analytiques

---

## Ã‰tape 6 : CrÃ©er le Semantic Model Power BI (FABRIC)

1. CrÃ©ez un nouveau **Semantic Model** dans Fabric
2. Connectez-le au Lakehouse (mode Direct Lake)
3. Importez les tables Gold
4. Configurez les relations et mesures DAX

---

## Ã‰tape 7 : Configurer le Data Agent (FABRIC)

1. CrÃ©ez un nouveau **Data Agent** dans Fabric
2. Connectez-le au Semantic Model
3. Testez avec des questions en langage naturel

---

## Checklist Rapide

- [x] 1ï¸âƒ£ DonnÃ©es gÃ©nÃ©rÃ©es localement (1.6M records)
- [ ] 2ï¸âƒ£ CSV uploadÃ©s dans Lakehouse Files/bronze/
- [ ] 3ï¸âƒ£ Notebook 01 - Ingestion Bronze (CSV â†’ Delta)
- [ ] 4ï¸âƒ£ Notebook 02 - Transformation Silver (QualitÃ©)
- [ ] 5ï¸âƒ£ Notebook 03 - Gold Star Schema (Analytics)
- [ ] 6ï¸âƒ£ Semantic Model Power BI crÃ©Ã©
- [ ] 7ï¸âƒ£ Data Agent configurÃ© et testÃ©

---

## Commandes de diagnostic

### VÃ©rifier les fichiers uploadÃ©s (dans un notebook Fabric)
```python
files = mssparkutils.fs.ls("Files/bronze/")
for f in files:
    print(f.name, f.size)
```

### VÃ©rifier les tables Bronze crÃ©Ã©es
```python
tables = spark.catalog.listTables()
bronze = [t for t in tables if not t.name.startswith(("silver_", "gold_"))]
print(f"Bronze tables: {len(bronze)}")
for t in bronze:
    print(f"  - {t.name}")
```

### VÃ©rifier les tables Silver crÃ©Ã©es
```python
silver = [t for t in spark.catalog.listTables() if t.name.startswith("silver_")]
print(f"Silver tables: {len(silver)}")
```

---

## Prochaine action recommandÃ©e

ğŸ¯ **Uploadez d'abord les CSV vers le Lakehouse**, puis exÃ©cutez les notebooks dans l'ordre !
