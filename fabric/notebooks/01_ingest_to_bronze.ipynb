{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c78c29c",
   "metadata": {},
   "source": [
    "# Bronze Layer Ingestion - Enterprise Data Platform\n",
    "\n",
    "## Overview\n",
    "This notebook ingests CSV files from OneLake Files into Delta tables (Bronze layer).\n",
    "\n",
    "**Prerequisites:**\n",
    "- CSV files uploaded to Lakehouse Files/bronze/\n",
    "- Lakehouse attached to this notebook\n",
    "\n",
    "**Output:**\n",
    "- Delta tables in Bronze layer\n",
    "- Data quality report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53708c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"Bronze Ingestion Started: {datetime.now()}\")\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1893899",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba454df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BRONZE_PATH = \"Files/bronze\"\n",
    "TABLE_PATH = \"Tables\"\n",
    "\n",
    "# List of tables to ingest (conformed dimensions)\n",
    "DIMENSION_TABLES = [\n",
    "    \"DimDate\",\n",
    "    \"DimCustomer\", \n",
    "    \"DimProduct\",\n",
    "    \"DimEmployee\",\n",
    "    \"DimGeography\",\n",
    "    \"DimFacility\",\n",
    "    \"DimProject\",\n",
    "    \"DimAccount\"\n",
    "]\n",
    "\n",
    "# List of fact tables (by domain)\n",
    "FACT_TABLES = {\n",
    "    \"Sales\": [\"FactSales\", \"FactReturns\"],\n",
    "    \"CRM\": [\"FactOpportunities\", \"FactActivities\"],\n",
    "    \"HR\": [\"FactAttrition\", \"FactHiring\"],\n",
    "    \"SupplyChain\": [\"FactInventory\", \"FactPurchaseOrders\"],\n",
    "    \"Manufacturing\": [\"FactProduction\", \"FactWorkOrders\"],\n",
    "    \"Finance\": [\"FactGeneralLedger\", \"FactBudget\"],\n",
    "    \"ESG\": [\"FactEmissions\"],\n",
    "    \"CallCenter\": [\"FactSupport\"],\n",
    "    \"ITOps\": [\"FactIncidents\"],\n",
    "    \"FinOps\": [\"FactCloudCosts\"],\n",
    "    \"RD\": [\"FactExperiments\"],\n",
    "    \"Quality\": [\"FactDefects\", \"FactSecurityEvents\"],\n",
    "    \"RiskCompliance\": [\"FactRisks\", \"FactAudits\", \"FactComplianceChecks\"]\n",
    "}\n",
    "\n",
    "print(f\"Bronze data source: {BRONZE_PATH}\")\n",
    "print(f\"Target: Delta tables in {TABLE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f4ec77",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa7503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_csv_to_delta(table_name: str, csv_path: str, overwrite: bool = True):\n",
    "    \"\"\"\n",
    "    Ingest CSV file to Delta table with schema inference.\n",
    "    \n",
    "    Args:\n",
    "        table_name: Name of the target Delta table\n",
    "        csv_path: Path to CSV file\n",
    "        overwrite: Whether to overwrite existing table\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Ingesting: {table_name}\")\n",
    "        print(f\"Source: {csv_path}\")\n",
    "        \n",
    "        # Read CSV with schema inference\n",
    "        df = spark.read.format(\"csv\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .option(\"dateFormat\", \"yyyy-MM-dd\") \\\n",
    "            .load(csv_path)\n",
    "        \n",
    "        row_count = df.count()\n",
    "        print(f\"Rows read: {row_count:,}\")\n",
    "        print(f\"Columns: {len(df.columns)}\")\n",
    "        \n",
    "        # Add metadata columns\n",
    "        df = df.withColumn(\"_ingestion_timestamp\", current_timestamp()) \\\n",
    "               .withColumn(\"_source_file\", lit(csv_path))\n",
    "        \n",
    "        # Write to Delta table\n",
    "        mode = \"overwrite\" if overwrite else \"append\"\n",
    "        df.write.format(\"delta\") \\\n",
    "            .mode(mode) \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .saveAsTable(table_name)\n",
    "        \n",
    "        print(f\"✅ {table_name} created successfully ({row_count:,} rows)\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error ingesting {table_name}: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d7c191",
   "metadata": {},
   "source": [
    "## Ingest Conformed Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e01f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest conformed dimensions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: Ingesting Conformed Dimensions\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "dimension_results = {}\n",
    "\n",
    "for table in DIMENSION_TABLES:\n",
    "    csv_path = f\"{BRONZE_PATH}/dimensions/{table}.csv\"\n",
    "    success = ingest_csv_to_delta(table, csv_path)\n",
    "    dimension_results[table] = success\n",
    "\n",
    "# Summary (use Python's built-in sum, not PySpark's)\n",
    "success_count = len([v for v in dimension_results.values() if v])\n",
    "print(f\"\\n✅ Dimensions ingested: {success_count}/{len(DIMENSION_TABLES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c235de4b",
   "metadata": {},
   "source": [
    "## Ingest Fact Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6a87c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest fact tables by domain\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: Ingesting Fact Tables\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fact_results = {}\n",
    "\n",
    "# Domain to folder mapping\n",
    "domain_folder_mapping = {\n",
    "    \"Sales\": \"sales\",\n",
    "    \"CRM\": \"crm\",\n",
    "    \"HR\": \"hr\",\n",
    "    \"SupplyChain\": \"supply_chain\",\n",
    "    \"Manufacturing\": \"manufacturing\",\n",
    "    \"Finance\": \"finance\",\n",
    "    \"ESG\": \"esg\",\n",
    "    \"CallCenter\": \"call_center\",\n",
    "    \"ITOps\": \"itops\",\n",
    "    \"FinOps\": \"finops\",\n",
    "    \"RD\": \"rd\",\n",
    "    \"Quality\": \"quality_security\",\n",
    "    \"RiskCompliance\": \"risk_compliance\"\n",
    "}\n",
    "\n",
    "# Updated FACT_TABLES with all domains\n",
    "FACT_TABLES_UPDATED = {\n",
    "    \"Sales\": [\"FactSales\", \"FactReturns\"],\n",
    "    \"CRM\": [\"FactOpportunities\", \"FactActivities\"],\n",
    "    \"HR\": [\"FactAttrition\", \"FactHiring\"],\n",
    "    \"SupplyChain\": [\"FactInventory\", \"FactPurchaseOrders\"],\n",
    "    \"Manufacturing\": [\"FactProduction\", \"FactWorkOrders\"],\n",
    "    \"Finance\": [\"FactGeneralLedger\", \"FactBudget\"],\n",
    "    \"ESG\": [\"FactEmissions\"],\n",
    "    \"CallCenter\": [\"FactSupport\"],\n",
    "    \"ITOps\": [\"FactIncidents\"],\n",
    "    \"FinOps\": [\"FactCloudCosts\"],\n",
    "    \"RD\": [\"FactExperiments\"],\n",
    "    \"Quality\": [\"FactDefects\", \"FactSecurityEvents\"],\n",
    "    \"RiskCompliance\": [\"FactRisks\", \"FactAudits\", \"FactComplianceChecks\"]\n",
    "}\n",
    "\n",
    "for domain, tables in FACT_TABLES_UPDATED.items():\n",
    "    print(f\"\\n--- {domain} Domain ---\")\n",
    "    folder = domain_folder_mapping.get(domain, domain.lower())\n",
    "    \n",
    "    for table in tables:\n",
    "        csv_path = f\"{BRONZE_PATH}/{folder}/{table}.csv\"\n",
    "        \n",
    "        # Check if file exists (some domains may not be generated yet)\n",
    "        try:\n",
    "            spark.read.format(\"csv\").option(\"header\", \"true\").load(csv_path).limit(1).count()\n",
    "            success = ingest_csv_to_delta(table, csv_path)\n",
    "            fact_results[table] = success\n",
    "        except Exception as e:\n",
    "            print(f\"⏭️  Skipping {table} (file not found)\")\n",
    "            fact_results[table] = None\n",
    "\n",
    "# Summary - use len() to avoid conflict with PySpark's sum()\n",
    "success_list = [v for v in fact_results.values() if v == True]\n",
    "skipped_list = [v for v in fact_results.values() if v is None]\n",
    "failed_list = [v for v in fact_results.values() if v == False]\n",
    "\n",
    "success_count = len(success_list)\n",
    "skipped_count = len(skipped_list)\n",
    "failed_count = len(failed_list)\n",
    "\n",
    "print(f\"\\n✅ Fact tables ingested: {success_count}\")\n",
    "print(f\"⏭️  Skipped (not generated): {skipped_count}\")\n",
    "print(f\"❌ Failed: {failed_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4972f838",
   "metadata": {},
   "source": [
    "## Verify Delta Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc9c5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all Delta tables created\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: Verification - Delta Tables Created\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tables = spark.catalog.listTables()\n",
    "delta_tables = [t for t in tables if t.tableType == \"MANAGED\"]\n",
    "\n",
    "print(f\"\\nTotal Delta tables: {len(delta_tables)}\\n\")\n",
    "\n",
    "# Display table statistics\n",
    "for table in sorted(delta_tables, key=lambda x: x.name):\n",
    "    df = spark.table(table.name)\n",
    "    row_count = df.count()\n",
    "    col_count = len(df.columns)\n",
    "    print(f\"  {table.name:30s} | {row_count:>10,} rows | {col_count:>3} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac32885",
   "metadata": {},
   "source": [
    "## Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39d3cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data quality checks\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: Data Quality Checks\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check for nulls in primary keys\n",
    "quality_issues = []\n",
    "\n",
    "# Check DimCustomer\n",
    "try:\n",
    "    dim_customer = spark.table(\"DimCustomer\")\n",
    "    null_count = dim_customer.filter(col(\"customer_id\").isNull()).count()\n",
    "    if null_count > 0:\n",
    "        quality_issues.append(f\"DimCustomer: {null_count} null customer_id values\")\n",
    "    else:\n",
    "        print(\"✅ DimCustomer: No null primary keys\")\n",
    "except:\n",
    "    print(\"⏭️  DimCustomer not available\")\n",
    "\n",
    "# Check DimProduct\n",
    "try:\n",
    "    dim_product = spark.table(\"DimProduct\")\n",
    "    null_count = dim_product.filter(col(\"product_id\").isNull()).count()\n",
    "    if null_count > 0:\n",
    "        quality_issues.append(f\"DimProduct: {null_count} null product_id values\")\n",
    "    else:\n",
    "        print(\"✅ DimProduct: No null primary keys\")\n",
    "except:\n",
    "    print(\"⏭️  DimProduct not available\")\n",
    "\n",
    "# Check FactSales\n",
    "try:\n",
    "    fact_sales = spark.table(\"FactSales\")\n",
    "    null_count = fact_sales.filter(\n",
    "        col(\"order_id\").isNull() | col(\"customer_id\").isNull() | col(\"product_id\").isNull()\n",
    "    ).count()\n",
    "    if null_count > 0:\n",
    "        quality_issues.append(f\"FactSales: {null_count} null key values\")\n",
    "    else:\n",
    "        print(\"✅ FactSales: No null key columns\")\n",
    "except:\n",
    "    print(\"⏭️  FactSales not available\")\n",
    "\n",
    "# Summary\n",
    "if quality_issues:\n",
    "    print(\"\\n⚠️  Data Quality Issues Found:\")\n",
    "    for issue in quality_issues:\n",
    "        print(f\"  - {issue}\")\n",
    "else:\n",
    "    print(\"\\n✅ All data quality checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d5f50a",
   "metadata": {},
   "source": [
    "## Completion Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9e7fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BRONZE INGESTION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Completion Time: {datetime.now()}\")\n",
    "\n",
    "# Calculate counts using len() to avoid PySpark sum() conflict\n",
    "dim_count = len([v for v in dimension_results.values() if v])\n",
    "fact_count = len([v for v in fact_results.values() if v == True])\n",
    "\n",
    "print(f\"\\nDimensions: {dim_count} tables\")\n",
    "print(f\"Facts: {fact_count} tables\")\n",
    "print(f\"\\nNext Step: Run notebook 02_transform_to_silver.ipynb\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
