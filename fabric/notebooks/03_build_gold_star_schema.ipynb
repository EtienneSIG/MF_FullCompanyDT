{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23a0ba03",
   "metadata": {},
   "source": [
    "# Gold Layer - Star Schema Builder\n",
    "\n",
    "## Overview\n",
    "This notebook builds analytics-ready star schemas (Gold layer) from Silver tables.\n",
    "\n",
    "**Star Schemas Created:**\n",
    "- Conformed Dimensions (DimDate, DimCustomer, DimProduct, DimEmployee)\n",
    "- Fact Tables optimized for Direct Lake\n",
    "\n",
    "**Prerequisites:**\n",
    "- Silver tables created (run 02_transform_to_silver.ipynb first)\n",
    "\n",
    "**Output:**\n",
    "- Gold star schema tables ready for Power BI Direct Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc890c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"Gold Star Schema Builder Started: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3eb585",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d516b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Naming for Gold layer (no prefix, ready for Power BI)\n",
    "DIMENSION_MAPPINGS = {\n",
    "    \"DimDate\": \"DimDate\",  # Already in final form from Bronze\n",
    "    \"Silver_DimCustomer\": \"DimCustomer\",\n",
    "    \"Silver_DimProduct\": \"DimProduct\",\n",
    "    \"Silver_DimEmployee\": \"DimEmployee\",\n",
    "    \"DimGeography\": \"DimGeography\"  # Already in final form\n",
    "}\n",
    "\n",
    "FACT_MAPPINGS = {\n",
    "    \"Silver_FactSales\": \"FactSales\",\n",
    "    \"Silver_FactSupport\": \"FactSupport\",\n",
    "    \"Silver_FactAttrition\": \"FactAttrition\",\n",
    "    \"Silver_FactInventory\": \"FactInventory\"\n",
    "}\n",
    "\n",
    "print(f\"Building {len(DIMENSION_MAPPINGS)} dimension tables\")\n",
    "print(f\"Building {len(FACT_MAPPINGS)} fact tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46121512",
   "metadata": {},
   "source": [
    "## Build Conformed Dimensions (Gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81a7c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: Building Conformed Dimensions (Gold)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for source_table, target_table in DIMENSION_MAPPINGS.items():\n",
    "    try:\n",
    "        print(f\"\\nBuilding {target_table}...\")\n",
    "        \n",
    "        # Read source (Bronze for DimDate/DimGeography, Silver for others)\n",
    "        df = spark.table(source_table)\n",
    "        \n",
    "        # Select only business columns (exclude metadata)\n",
    "        metadata_cols = [\"_ingestion_timestamp\", \"_source_file\", \"row_num\"]\n",
    "        business_cols = [c for c in df.columns if c not in metadata_cols]\n",
    "        df_clean = df.select(*business_cols)\n",
    "        \n",
    "        # Add SCD Type 1 effective date (for future SCD Type 2 implementation)\n",
    "        if \"effective_date\" not in df_clean.columns:\n",
    "            df_clean = df_clean.withColumn(\"effective_date\", current_date())\n",
    "        \n",
    "        row_count = df_clean.count()\n",
    "        \n",
    "        # Write to Gold layer (overwrite mode for full refresh)\n",
    "        df_clean.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .saveAsTable(target_table)\n",
    "        \n",
    "        print(f\"âœ… {target_table} created: {row_count:,} rows\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Skipping {target_table}: {str(e)}\")\n",
    "\n",
    "print(\"\\nâœ… Conformed dimensions created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dbb57f",
   "metadata": {},
   "source": [
    "## Build Fact Tables (Gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8834aef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: Building Fact Tables (Gold)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for source_table, target_table in FACT_MAPPINGS.items():\n",
    "    try:\n",
    "        print(f\"\\nBuilding {target_table}...\")\n",
    "        \n",
    "        # Read Silver table\n",
    "        df = spark.table(source_table)\n",
    "        \n",
    "        # Select only business columns\n",
    "        metadata_cols = [\"_ingestion_timestamp\", \"_source_file\", \"row_num\"]\n",
    "        business_cols = [c for c in df.columns if c not in metadata_cols]\n",
    "        df_clean = df.select(*business_cols)\n",
    "        \n",
    "        # Add partition column for performance (date-based)\n",
    "        if \"order_date_id\" in df_clean.columns:\n",
    "            df_clean = df_clean.withColumn(\"year_month\", \n",
    "                                           substring(col(\"order_date_id\").cast(\"string\"), 1, 6))\n",
    "        elif \"create_date_id\" in df_clean.columns:\n",
    "            df_clean = df_clean.withColumn(\"year_month\",\n",
    "                                           substring(col(\"create_date_id\").cast(\"string\"), 1, 6))\n",
    "        \n",
    "        row_count = df_clean.count()\n",
    "        \n",
    "        # Write to Gold layer with partitioning (if applicable)\n",
    "        if \"year_month\" in df_clean.columns:\n",
    "            df_clean.write.format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .partitionBy(\"year_month\") \\\n",
    "                .saveAsTable(target_table)\n",
    "        else:\n",
    "            df_clean.write.format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .saveAsTable(target_table)\n",
    "        \n",
    "        print(f\"âœ… {target_table} created: {row_count:,} rows\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Skipping {target_table}: {str(e)}\")\n",
    "\n",
    "print(\"\\nâœ… Fact tables created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a109fc9f",
   "metadata": {},
   "source": [
    "## Optimize Delta Tables for Direct Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4149979",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: Optimizing Delta Tables for Direct Lake\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get all Gold tables (Dim* and Fact*)\n",
    "gold_tables = [t.name for t in spark.catalog.listTables() \n",
    "               if t.name.startswith(\"Dim\") or t.name.startswith(\"Fact\")]\n",
    "\n",
    "for table_name in gold_tables:\n",
    "    try:\n",
    "        print(f\"\\nOptimizing {table_name}...\")\n",
    "        \n",
    "        # Run OPTIMIZE command to compact small files\n",
    "        spark.sql(f\"OPTIMIZE {table_name}\")\n",
    "        \n",
    "        # Run VACUUM to clean up old files (keep 7 days)\n",
    "        # Note: In production, adjust retention period as needed\n",
    "        spark.sql(f\"VACUUM {table_name} RETAIN 168 HOURS\")\n",
    "        \n",
    "        print(f\"âœ… {table_name} optimized\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Could not optimize {table_name}: {str(e)}\")\n",
    "\n",
    "print(\"\\nâœ… Delta table optimization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e35828",
   "metadata": {},
   "source": [
    "## Verify Star Schema Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d375cc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: Verifying Star Schema Relationships\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Verify FactSales â†’ DimCustomer relationship\n",
    "try:\n",
    "    fact_sales = spark.table(\"FactSales\")\n",
    "    dim_customer = spark.table(\"DimCustomer\")\n",
    "    \n",
    "    # Find orphaned customer_ids in FactSales\n",
    "    orphaned = fact_sales.select(\"customer_id\").distinct() \\\n",
    "        .join(dim_customer.select(\"customer_id\"), \"customer_id\", \"left_anti\")\n",
    "    \n",
    "    orphan_count = orphaned.count()\n",
    "    \n",
    "    if orphan_count == 0:\n",
    "        print(\"âœ… FactSales â†’ DimCustomer: All customer_ids valid\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  FactSales â†’ DimCustomer: {orphan_count} orphaned customer_ids\")\n",
    "        print(\"   Sample orphaned IDs:\")\n",
    "        orphaned.show(5, truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"â­ï¸  Skipping FactSales â†’ DimCustomer check: {str(e)}\")\n",
    "\n",
    "# Verify FactSales â†’ DimProduct relationship\n",
    "try:\n",
    "    fact_sales = spark.table(\"FactSales\")\n",
    "    dim_product = spark.table(\"DimProduct\")\n",
    "    \n",
    "    orphaned = fact_sales.select(\"product_id\").distinct() \\\n",
    "        .join(dim_product.select(\"product_id\"), \"product_id\", \"left_anti\")\n",
    "    \n",
    "    orphan_count = orphaned.count()\n",
    "    \n",
    "    if orphan_count == 0:\n",
    "        print(\"âœ… FactSales â†’ DimProduct: All product_ids valid\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  FactSales â†’ DimProduct: {orphan_count} orphaned product_ids\")\n",
    "except Exception as e:\n",
    "    print(f\"â­ï¸  Skipping FactSales â†’ DimProduct check: {str(e)}\")\n",
    "\n",
    "# Verify FactSales â†’ DimDate relationship\n",
    "try:\n",
    "    fact_sales = spark.table(\"FactSales\")\n",
    "    dim_date = spark.table(\"DimDate\")\n",
    "    \n",
    "    orphaned = fact_sales.select(\"order_date_id\").distinct() \\\n",
    "        .join(dim_date.select(\"date_id\"), \n",
    "              fact_sales.order_date_id == dim_date.date_id, \n",
    "              \"left_anti\")\n",
    "    \n",
    "    orphan_count = orphaned.count()\n",
    "    \n",
    "    if orphan_count == 0:\n",
    "        print(\"âœ… FactSales â†’ DimDate: All order_date_ids valid\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  FactSales â†’ DimDate: {orphan_count} orphaned order_date_ids\")\n",
    "except Exception as e:\n",
    "    print(f\"â­ï¸  Skipping FactSales â†’ DimDate check: {str(e)}\")\n",
    "\n",
    "print(\"\\nâœ… Referential integrity validation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0494c0",
   "metadata": {},
   "source": [
    "## Generate Star Schema Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916e0a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GOLD LAYER - STAR SCHEMA SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# List all Gold tables\n",
    "gold_dims = [t for t in spark.catalog.listTables() if t.name.startswith(\"Dim\")]\n",
    "gold_facts = [t for t in spark.catalog.listTables() if t.name.startswith(\"Fact\")]\n",
    "\n",
    "print(\"\\nðŸ“Š DIMENSION TABLES\")\n",
    "print(\"-\" * 80)\n",
    "for table in sorted(gold_dims, key=lambda x: x.name):\n",
    "    df = spark.table(table.name)\n",
    "    row_count = df.count()\n",
    "    col_count = len(df.columns)\n",
    "    print(f\"  {table.name:20s} | {row_count:>10,} rows | {col_count:>3} columns\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ FACT TABLES\")\n",
    "print(\"-\" * 80)\n",
    "for table in sorted(gold_facts, key=lambda x: x.name):\n",
    "    df = spark.table(table.name)\n",
    "    row_count = df.count()\n",
    "    col_count = len(df.columns)\n",
    "    \n",
    "    # Show partitioning info if available\n",
    "    partition_info = \"\"\n",
    "    if \"year_month\" in df.columns:\n",
    "        partition_count = df.select(\"year_month\").distinct().count()\n",
    "        partition_info = f\"| {partition_count} partitions\"\n",
    "    \n",
    "    print(f\"  {table.name:20s} | {row_count:>10,} rows | {col_count:>3} columns {partition_info}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Completion Time: {datetime.now()}\")\n",
    "print(\"\\nâœ… Gold star schema is ready for Power BI Direct Lake!\")\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  1. Create Power BI semantic model with Direct Lake connection\")\n",
    "print(\"  2. Add relationships in Power BI model view\")\n",
    "print(\"  3. Import DAX measures from fabric/powerbi/dax-measures.md\")\n",
    "print(\"  4. Configure Fabric Data Agent\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
