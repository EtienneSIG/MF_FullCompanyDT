{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23a0ba03",
   "metadata": {},
   "source": [
    "# Gold Layer - Star Schema Builder\n",
    "\n",
    "## Overview\n",
    "This notebook builds analytics-ready star schemas (Gold layer) from Silver tables.\n",
    "\n",
    "**Star Schemas Created:**\n",
    "- Conformed Dimensions (DimDate, DimCustomer, DimProduct, DimEmployee)\n",
    "- Fact Tables optimized for Direct Lake\n",
    "\n",
    "**Prerequisites:**\n",
    "- Silver tables created (run 02_transform_to_silver.ipynb first)\n",
    "\n",
    "**Output:**\n",
    "- Gold star schema tables ready for Power BI Direct Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc890c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"Gold Star Schema Builder Started: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d88da4",
   "metadata": {},
   "source": [
    "## üîç Diagnostic - Prerequisites Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66026ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DIAGNOSTIC - Check prerequisites for Gold layer creation\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DIAGNOSTIC - Checking prerequisites for Gold layer\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. List all tables in catalog\n",
    "all_tables_list = spark.catalog.listTables()\n",
    "print(f\"\\n1Ô∏è‚É£ Total tables in catalog: {len(all_tables_list)}\")\n",
    "\n",
    "if len(all_tables_list) == 0:\n",
    "    print(\"\\n‚ùå ERROR: No tables found in lakehouse!\")\n",
    "    print(\"   ACTION REQUIRED:\")\n",
    "    print(\"   1. Upload CSV files to Files/bronze/\")\n",
    "    print(\"   2. Run notebook 01_ingest_to_bronze.ipynb\")\n",
    "    print(\"   3. Run notebook 02_transform_to_silver.ipynb\")\n",
    "else:\n",
    "    # 2. Check Bronze dimensions\n",
    "    bronze_dims = [t.name for t in all_tables_list \n",
    "                   if t.name.lower().startswith(\"dim\") \n",
    "                   and not t.name.lower().startswith(\"silver_\")\n",
    "                   and not t.name.lower().startswith(\"gold_\")]\n",
    "    \n",
    "    print(f\"\\n2Ô∏è‚É£ Bronze dimensions found: {len(bronze_dims)}\")\n",
    "    if bronze_dims:\n",
    "        for dim in sorted(bronze_dims):\n",
    "            df = spark.table(dim)\n",
    "            print(f\"   ‚úÖ {dim:25s} ({df.count():,} rows)\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  No Bronze dimensions found\")\n",
    "    \n",
    "    # 3. Check Silver dimensions\n",
    "    silver_dims = [t.name for t in all_tables_list \n",
    "                   if t.name.lower().startswith(\"silver_dim\")]\n",
    "    \n",
    "    print(f\"\\n3Ô∏è‚É£ Silver dimensions found: {len(silver_dims)}\")\n",
    "    if silver_dims:\n",
    "        for dim in sorted(silver_dims):\n",
    "            df = spark.table(dim)\n",
    "            print(f\"   ‚úÖ {dim:35s} ({df.count():,} rows)\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  No Silver dimensions found\")\n",
    "        print(\"   ACTION: Run notebook 02_transform_to_silver.ipynb first\")\n",
    "    \n",
    "    # 4. Check Silver facts\n",
    "    silver_facts = [t.name for t in all_tables_list \n",
    "                    if t.name.lower().startswith(\"silver_fact\")]\n",
    "    \n",
    "    print(f\"\\n4Ô∏è‚É£ Silver facts found: {len(silver_facts)}\")\n",
    "    if silver_facts:\n",
    "        for i, fact in enumerate(sorted(silver_facts), 1):\n",
    "            df = spark.table(fact)\n",
    "            print(f\"   {i:2d}. {fact:35s} ({df.count():,} rows)\")\n",
    "            if i >= 10:\n",
    "                remaining = len(silver_facts) - 10\n",
    "                if remaining > 0:\n",
    "                    print(f\"   ... and {remaining} more\")\n",
    "                break\n",
    "    \n",
    "    # 5. Check existing Gold tables\n",
    "    gold_dims = [t.name for t in all_tables_list \n",
    "                 if t.name.lower().startswith(\"gold_dim\")]\n",
    "    gold_facts = [t.name for t in all_tables_list \n",
    "                  if t.name.lower().startswith(\"gold_fact\")]\n",
    "    \n",
    "    print(f\"\\n5Ô∏è‚É£ Existing Gold tables: {len(gold_dims) + len(gold_facts)}\")\n",
    "    if gold_dims or gold_facts:\n",
    "        print(\"   ‚ö†Ô∏è  Gold tables already exist (will be overwritten):\")\n",
    "        for dim in sorted(gold_dims):\n",
    "            print(f\"     {dim}\")\n",
    "        for fact in sorted(gold_facts[:5]):\n",
    "            print(f\"     {fact}\")\n",
    "        if len(gold_facts) > 5:\n",
    "            print(f\"     ... and {len(gold_facts) - 5} more facts\")\n",
    "    else:\n",
    "        print(\"   ‚ÑπÔ∏è  No Gold tables yet (this is expected on first run)\")\n",
    "    \n",
    "    # 6. Show what will be created\n",
    "    total_dims = len(bronze_dims) + len(silver_dims)\n",
    "    total_facts = len(silver_facts)\n",
    "    \n",
    "    print(f\"\\n6Ô∏è‚É£ Tables to be created:\")\n",
    "    print(f\"   üìä {total_dims} Gold dimensions (from {len(bronze_dims)} Bronze + {len(silver_dims)} Silver)\")\n",
    "    print(f\"   üìà {total_facts} Gold fact tables\")\n",
    "    \n",
    "    # 7. Final status\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    if len(bronze_dims) > 0 or len(silver_dims) > 0:\n",
    "        print(\"‚úÖ READY - Prerequisites met. You can proceed with Gold layer creation.\")\n",
    "    else:\n",
    "        print(\"‚ùå NOT READY - Missing dimension tables.\")\n",
    "        print(\"   Run notebooks 01 and 02 first to create Bronze and Silver tables.\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3eb585",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d516b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Dynamically discover Silver tables and Bronze dimensions\n",
    "all_tables = spark.catalog.listTables()\n",
    "\n",
    "print(f\"Total tables in catalog: {len(all_tables)}\")\n",
    "\n",
    "if len(all_tables) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: No tables found in lakehouse!\")\n",
    "    print(\"Please run notebook 02_transform_to_silver.ipynb first.\")\n",
    "    DIMENSION_MAPPINGS = {}\n",
    "    FACT_MAPPINGS = {}\n",
    "else:\n",
    "        \n",
    "    # Get Silver dimensions\n",
    "    silver_dims = [t.name for t in all_tables if t.name.startswith(\"silver_dim\")]\n",
    "    \n",
    "    # Get Silver facts\n",
    "    silver_facts = [t.name for t in all_tables if t.name.startswith(\"silver_fact\")]\n",
    "    \n",
    "    # Build dimension mappings (Bronze Dim* ‚Üí Gold Dim*, Silver_Dim* ‚Üí Gold Dim*)\n",
    "    DIMENSION_MAPPINGS = {}\n",
    "    \n",
    "    # Add Bronze dimensions (DimDate, DimGeography) with Gold_ prefix\n",
    "    for bronze_dim in bronze_dims:\n",
    "        gold_name = f\"Gold_{bronze_dim}\"  # Add Gold_ prefix\n",
    "        DIMENSION_MAPPINGS[bronze_dim] = gold_name\n",
    "    \n",
    "    # Add Silver dimensions (replace silver_ with Gold_)\n",
    "    for silver_dim in silver_dims:\n",
    "        gold_name = silver_dim.replace(\"silver_\", \"Gold_\")\n",
    "        DIMENSION_MAPPINGS[silver_dim] = gold_name\n",
    "    \n",
    "    # Build fact mappings (replace silver_ with Gold_)\n",
    "    FACT_MAPPINGS = {}\n",
    "    for silver_fact in silver_facts:\n",
    "        gold_name = silver_fact.replace(\"silver_\", \"Gold_\")\n",
    "        FACT_MAPPINGS[silver_fact] = gold_name\n",
    "    \n",
    "    print(f\"\\n‚úÖ Discovered {len(DIMENSION_MAPPINGS)} dimension tables to build\")\n",
    "    print(f\"‚úÖ Discovered {len(FACT_MAPPINGS)} fact tables to build\")\n",
    "    \n",
    "    print(f\"\\nDimensions ({len(DIMENSION_MAPPINGS)}):\")\n",
    "    for source, target in sorted(DIMENSION_MAPPINGS.items()):\n",
    "        print(f\"  {source:35s} ‚Üí {target}\")\n",
    "    \n",
    "    print(f\"\\nFacts ({len(FACT_MAPPINGS)}):\")\n",
    "    for i, (source, target) in enumerate(sorted(FACT_MAPPINGS.items()), 1):\n",
    "        print(f\"  {i:2d}. {source:35s} ‚Üí {target}\")\n",
    "        if i >= 15:\n",
    "            remaining = len(FACT_MAPPINGS) - 15\n",
    "            if remaining > 0:\n",
    "                print(f\"  ... and {remaining} more\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46121512",
   "metadata": {},
   "source": [
    "## Build Conformed Dimensions (Gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81a7c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: Building Dimension Tables (Gold)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "dimension_results = {}\n",
    "\n",
    "for source_table, target_table in DIMENSION_MAPPINGS.items():\n",
    "    try:\n",
    "        print(f\"\\nBuilding {target_table}...\")\n",
    "        \n",
    "        # Read source table (Bronze or Silver)\n",
    "        df = spark.table(source_table)\n",
    "        \n",
    "        # Remove metadata columns if present\n",
    "        metadata_cols = [\"_ingestion_timestamp\", \"_source_file\", \"row_num\"]\n",
    "        business_cols = [c for c in df.columns if c not in metadata_cols]\n",
    "        df_clean = df.select(*business_cols)\n",
    "        \n",
    "        row_count = df_clean.count()\n",
    "        \n",
    "        # Write to Gold layer\n",
    "        df_clean.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .saveAsTable(target_table)\n",
    "        \n",
    "        print(f\"‚úÖ {target_table} created: {row_count:,} rows\")\n",
    "        dimension_results[target_table] = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating {target_table}: {str(e)}\")\n",
    "        dimension_results[target_table] = False\n",
    "\n",
    "success_count = len([v for v in dimension_results.values() if v])\n",
    "print(f\"\\n‚úÖ Dimension tables created: {success_count}/{len(DIMENSION_MAPPINGS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dbb57f",
   "metadata": {},
   "source": [
    "## Build Fact Tables (Gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8834aef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: Building Fact Tables (Gold)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for source_table, target_table in FACT_MAPPINGS.items():\n",
    "    try:\n",
    "        print(f\"\\nBuilding {target_table}...\")\n",
    "        \n",
    "        # Read Silver table\n",
    "        df = spark.table(source_table)\n",
    "        \n",
    "        # Select only business columns\n",
    "        metadata_cols = [\"_ingestion_timestamp\", \"_source_file\", \"row_num\"]\n",
    "        business_cols = [c for c in df.columns if c not in metadata_cols]\n",
    "        df_clean = df.select(*business_cols)\n",
    "        \n",
    "        # Add partition column for performance (date-based)\n",
    "        if \"order_date_id\" in df_clean.columns:\n",
    "            df_clean = df_clean.withColumn(\"year_month\", \n",
    "                                           substring(col(\"order_date_id\").cast(\"string\"), 1, 6))\n",
    "        elif \"create_date_id\" in df_clean.columns:\n",
    "            df_clean = df_clean.withColumn(\"year_month\",\n",
    "                                           substring(col(\"create_date_id\").cast(\"string\"), 1, 6))\n",
    "        \n",
    "        row_count = df_clean.count()\n",
    "        \n",
    "        # Write to Gold layer with partitioning (if applicable)\n",
    "        if \"year_month\" in df_clean.columns:\n",
    "            df_clean.write.format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .partitionBy(\"year_month\") \\\n",
    "                .saveAsTable(target_table)\n",
    "        else:\n",
    "            df_clean.write.format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .saveAsTable(target_table)\n",
    "        \n",
    "        print(f\"‚úÖ {target_table} created: {row_count:,} rows\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Skipping {target_table}: {str(e)}\")\n",
    "\n",
    "print(\"\\n‚úÖ Fact tables created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a109fc9f",
   "metadata": {},
   "source": [
    "## Optimize Delta Tables for Direct Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4149979",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: Optimizing Delta Tables for Direct Lake\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get all Gold tables (Dim* and Fact*) - case insensitive\n",
    "gold_tables = [t.name for t in spark.catalog.listTables() \n",
    "               if t.name.lower().startswith(\"gold_dim\") or t.name.lower().startswith(\"gold_fact\")]\n",
    "\n",
    "for table_name in gold_tables:\n",
    "    try:\n",
    "        print(f\"\\nOptimizing {table_name}...\")\n",
    "        \n",
    "        # Run OPTIMIZE command to compact small files\n",
    "        spark.sql(f\"OPTIMIZE {table_name}\")\n",
    "        \n",
    "        # Run VACUUM to clean up old files (keep 7 days)\n",
    "        # Note: In production, adjust retention period as needed\n",
    "        spark.sql(f\"VACUUM {table_name} RETAIN 168 HOURS\")\n",
    "        \n",
    "        print(f\"‚úÖ {table_name} optimized\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not optimize {table_name}: {str(e)}\")\n",
    "\n",
    "print(\"\\n‚úÖ Delta table optimization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e35828",
   "metadata": {},
   "source": [
    "## Verify Star Schema Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d375cc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: Verifying Star Schema Relationships\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get all Gold fact and dimension tables - case insensitive\n",
    "gold_facts = [t.name for t in spark.catalog.listTables() \n",
    "              if t.name.lower().startswith(\"gold_fact\")]\n",
    "gold_dims = [t.name for t in spark.catalog.listTables() \n",
    "             if t.name.lower().startswith(\"gold_dim\") or t.name.lower().startswith(\"dim\")]\n",
    "\n",
    "print(f\"\\nüìä Found {len(gold_facts)} fact tables and {len(gold_dims)} dimension tables\")\n",
    "\n",
    "# Track overall validation results\n",
    "total_checks = 0\n",
    "passed_checks = 0\n",
    "failed_checks = 0\n",
    "\n",
    "# For each fact table, verify all foreign key relationships\n",
    "for fact_table_name in sorted(gold_facts):\n",
    "    try:\n",
    "        fact_df = spark.table(fact_table_name)\n",
    "        fact_columns = fact_df.columns\n",
    "        \n",
    "        print(f\"\\nüîç Checking {fact_table_name}...\")\n",
    "        \n",
    "        # Find all potential foreign key columns (ending with _id or _key)\n",
    "        fk_columns = [col for col in fact_columns \n",
    "                     if col.endswith('_id') or col.endswith('_key')]\n",
    "        \n",
    "        if not fk_columns:\n",
    "            print(f\"   ‚ÑπÔ∏è  No foreign key columns found\")\n",
    "            continue\n",
    "        \n",
    "        # For each foreign key column, try to find matching dimension table\n",
    "        for fk_col in fk_columns:\n",
    "            # Try to infer dimension table name\n",
    "            # Examples: customer_id ‚Üí gold_dimcustomer, product_id ‚Üí gold_dimproduct\n",
    "            # date_id ‚Üí dimdate, order_date_id ‚Üí dimdate\n",
    "            \n",
    "            possible_dim_names = []\n",
    "            \n",
    "            # Extract base name from FK column\n",
    "            if fk_col.endswith('_date_id'):\n",
    "                possible_dim_names = ['dimdate', 'gold_dimdate']\n",
    "            elif fk_col.endswith('_id'):\n",
    "                base_name = fk_col.replace('_id', '')\n",
    "                possible_dim_names = [\n",
    "                    f'gold_dim{base_name}',\n",
    "                    f'dim{base_name}',\n",
    "                    f'gold_dim{base_name.replace(\"_\", \"\")}',\n",
    "                    f'dim{base_name.replace(\"_\", \"\")}'\n",
    "                ]\n",
    "            elif fk_col.endswith('_key'):\n",
    "                base_name = fk_col.replace('_key', '')\n",
    "                possible_dim_names = [\n",
    "                    f'gold_dim{base_name}',\n",
    "                    f'dim{base_name}',\n",
    "                    f'gold_dim{base_name.replace(\"_\", \"\")}',\n",
    "                    f'dim{base_name.replace(\"_\", \"\")}'\n",
    "                ]\n",
    "            \n",
    "            # Find matching dimension table\n",
    "            matching_dim = None\n",
    "            for dim_name in possible_dim_names:\n",
    "                if dim_name in [d.lower() for d in gold_dims]:\n",
    "                    matching_dim = [d for d in gold_dims if d.lower() == dim_name][0]\n",
    "                    break\n",
    "            \n",
    "            if not matching_dim:\n",
    "                # Check if the FK might reference a dimension with different naming\n",
    "                for dim_table in gold_dims:\n",
    "                    dim_df = spark.table(dim_table)\n",
    "                    if fk_col in dim_df.columns:\n",
    "                        matching_dim = dim_table\n",
    "                        break\n",
    "            \n",
    "            if matching_dim:\n",
    "                total_checks += 1\n",
    "                try:\n",
    "                    # Get dimension table\n",
    "                    dim_df = spark.table(matching_dim)\n",
    "                    \n",
    "                    # Find the primary key column in dimension\n",
    "                    # Try common patterns: same name as FK, or without prefix\n",
    "                    pk_candidates = [fk_col]\n",
    "                    if '_' in fk_col:\n",
    "                        # For order_date_id, try date_id\n",
    "                        parts = fk_col.split('_')\n",
    "                        if len(parts) > 2:\n",
    "                            pk_candidates.append('_'.join(parts[-2:]))\n",
    "                    \n",
    "                    pk_col = None\n",
    "                    for candidate in pk_candidates:\n",
    "                        if candidate in dim_df.columns:\n",
    "                            pk_col = candidate\n",
    "                            break\n",
    "                    \n",
    "                    if not pk_col:\n",
    "                        print(f\"   ‚ö†Ô∏è  {fk_col} ‚Üí {matching_dim}: Cannot find PK column\")\n",
    "                        failed_checks += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Check for orphaned records\n",
    "                    orphaned = fact_df.select(fk_col).distinct() \\\n",
    "                        .join(dim_df.select(pk_col), \n",
    "                              fact_df[fk_col] == dim_df[pk_col], \n",
    "                              \"left_anti\")\n",
    "                    \n",
    "                    orphan_count = orphaned.count()\n",
    "                    \n",
    "                    if orphan_count == 0:\n",
    "                        print(f\"   ‚úÖ {fk_col} ‚Üí {matching_dim}.{pk_col}: All keys valid\")\n",
    "                        passed_checks += 1\n",
    "                    else:\n",
    "                        print(f\"   ‚ö†Ô∏è  {fk_col} ‚Üí {matching_dim}.{pk_col}: {orphan_count} orphaned keys\")\n",
    "                        failed_checks += 1\n",
    "                        if orphan_count <= 5:\n",
    "                            print(f\"       Orphaned values:\")\n",
    "                            orphaned.show(orphan_count, truncate=False)\n",
    "                        else:\n",
    "                            print(f\"       Sample orphaned values:\")\n",
    "                            orphaned.show(5, truncate=False)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ùå Error checking {fk_col} ‚Üí {matching_dim}: {str(e)}\")\n",
    "                    failed_checks += 1\n",
    "            else:\n",
    "                # FK column doesn't match any dimension table\n",
    "                print(f\"   ‚ÑπÔ∏è  {fk_col}: No matching dimension table found\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {fact_table_name}: {str(e)}\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REFERENTIAL INTEGRITY VALIDATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total relationship checks: {total_checks}\")\n",
    "print(f\"‚úÖ Passed: {passed_checks}\")\n",
    "print(f\"‚ö†Ô∏è  Failed: {failed_checks}\")\n",
    "\n",
    "if failed_checks == 0 and total_checks > 0:\n",
    "    print(\"\\nüéâ All referential integrity checks passed!\")\n",
    "elif total_checks == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  No relationships could be validated\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  {failed_checks} relationship(s) have orphaned records\")\n",
    "    print(\"   Consider cleaning data or updating dimension tables\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0494c0",
   "metadata": {},
   "source": [
    "## Generate Star Schema Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916e0a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GOLD LAYER - STAR SCHEMA SUMMARY\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# List all Gold tables\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m gold_dims = [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[43mspark\u001b[49m.catalog.listTables() \u001b[38;5;28;01mif\u001b[39;00m t.name.startswith(\u001b[33m\"\u001b[39m\u001b[33mDim\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m      7\u001b[39m gold_facts = [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m spark.catalog.listTables() \u001b[38;5;28;01mif\u001b[39;00m t.name.startswith(\u001b[33m\"\u001b[39m\u001b[33mFact\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìä DIMENSION TABLES\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GOLD LAYER - STAR SCHEMA SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get all Gold tables dynamically\n",
    "all_gold_tables = spark.catalog.listTables()\n",
    "\n",
    "# Separate dimensions and facts\n",
    "gold_dims = [t for t in all_gold_tables \n",
    "             if t.name.startswith(\"gold_dim\") or \n",
    "                (t.name.startswith(\"gold_dim\") and not t.name.startswith(\"gold_\"))]\n",
    "gold_facts = [t for t in all_gold_tables if t.name.startswith(\"gold_fact\")]\n",
    "\n",
    "# Calculate total statistics\n",
    "total_dim_rows = 0\n",
    "total_fact_rows = 0\n",
    "\n",
    "print(\"\\nüìä DIMENSION TABLES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if len(gold_dims) == 0:\n",
    "    print(\"  ‚ö†Ô∏è  No dimension tables found\")\n",
    "else:\n",
    "    print(f\"{'Table Name':<30} | {'Rows':>12} | {'Columns':>8} | {'Size Info':<20}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for table in sorted(gold_dims, key=lambda x: x.name):\n",
    "        try:\n",
    "            df = spark.table(table.name)\n",
    "            row_count = df.count()\n",
    "            col_count = len(df.columns)\n",
    "            total_dim_rows += row_count\n",
    "            \n",
    "            # Get some key column info\n",
    "            key_cols = [c for c in df.columns if c.endswith('_id') or c.endswith('_key')]\n",
    "            key_info = f\"{len(key_cols)} key col(s)\"\n",
    "            \n",
    "            print(f\"  {table.name:<28} | {row_count:>12,} | {col_count:>8} | {key_info:<20}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {table.name:<28} | {'ERROR':>12} | {'-':>8} | {str(e)[:20]:<20}\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  {'TOTAL DIMENSIONS':<28} | {total_dim_rows:>12,} | {'':<8} | {len(gold_dims)} table(s)\")\n",
    "\n",
    "print(\"\\nüìà FACT TABLES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if len(gold_facts) == 0:\n",
    "    print(\"  ‚ö†Ô∏è  No fact tables found\")\n",
    "else:\n",
    "    print(f\"{'Table Name':<30} | {'Rows':>12} | {'Columns':>8} | {'Partitions':<20}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for table in sorted(gold_facts, key=lambda x: x.name):\n",
    "        try:\n",
    "            df = spark.table(table.name)\n",
    "            row_count = df.count()\n",
    "            col_count = len(df.columns)\n",
    "            total_fact_rows += row_count\n",
    "            \n",
    "            # Check for partitioning\n",
    "            partition_info = \"Not partitioned\"\n",
    "            if \"year_month\" in df.columns:\n",
    "                partition_count = df.select(\"year_month\").distinct().count()\n",
    "                partition_info = f\"{partition_count} partition(s)\"\n",
    "            \n",
    "            print(f\"  {table.name:<28} | {row_count:>12,} | {col_count:>8} | {partition_info:<20}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {table.name:<28} | {'ERROR':>12} | {'-':>8} | {str(e)[:20]:<20}\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  {'TOTAL FACTS':<28} | {total_fact_rows:>12,} | {'':<8} | {len(gold_facts)} table(s)\")\n",
    "\n",
    "# Overall summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Total Dimension Tables: {len(gold_dims)}\")\n",
    "print(f\"  Total Fact Tables:      {len(gold_facts)}\")\n",
    "print(f\"  Total Dimension Rows:   {total_dim_rows:,}\")\n",
    "print(f\"  Total Fact Rows:        {total_fact_rows:,}\")\n",
    "print(f\"  Total Gold Tables:      {len(gold_dims) + len(gold_facts)}\")\n",
    "\n",
    "if len(gold_dims) > 0 and len(gold_facts) > 0:\n",
    "    avg_cardinality = total_fact_rows / total_dim_rows if total_dim_rows > 0 else 0\n",
    "    print(f\"  Avg Fact:Dim Ratio:     {avg_cardinality:.2f}:1\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Completion Time: {datetime.now()}\")\n",
    "\n",
    "# Final status message\n",
    "if len(gold_dims) > 0 and len(gold_facts) > 0:\n",
    "    print(\"\\n‚úÖ Gold star schema is ready for Power BI Direct Lake!\")\n",
    "    print(\"\\nNext Steps:\")\n",
    "    print(\"  1. Create Power BI semantic model with Direct Lake connection\")\n",
    "    print(\"  2. Add relationships in Power BI model view\")\n",
    "    print(\"  3. Import DAX measures from fabric/powerbi/dax-measures.md\")\n",
    "    print(\"  4. Configure Fabric Data Agent\")\n",
    "elif len(gold_facts) > 0 and len(gold_dims) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Fact tables created but no dimension tables found!\")\n",
    "    print(\"   Consider running the dimension creation notebook first.\")\n",
    "elif len(gold_dims) > 0 and len(gold_facts) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Dimension tables created but no fact tables found!\")\n",
    "    print(\"   Consider running the Silver layer notebook to create fact tables.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: No Gold tables were created!\")\n",
    "    print(\"   Please check the previous steps for errors.\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
