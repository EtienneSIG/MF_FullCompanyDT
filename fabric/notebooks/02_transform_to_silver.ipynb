{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33d9e753",
   "metadata": {},
   "source": [
    "# Silver Layer Transformation - Enterprise Data Platform\n",
    "\n",
    "## Overview\n",
    "This notebook transforms Bronze Delta tables into Silver layer with:\n",
    "- Data quality improvements\n",
    "- Data type standardization\n",
    "- Deduplication\n",
    "- Conformance to business rules\n",
    "\n",
    "**Prerequisites:**\n",
    "- Bronze Delta tables created (run 01_ingest_to_bronze.ipynb first)\n",
    "\n",
    "**Output:**\n",
    "- Silver Delta tables with cleaned, conformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aced21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"Silver Transformation Started: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f75a26",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deec9d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SILVER_PREFIX = \"Silver_\"\n",
    "\n",
    "# Get all tables from catalog\n",
    "all_tables = spark.catalog.listTables()\n",
    "\n",
    "print(f\"Total tables in catalog: {len(all_tables)}\")\n",
    "\n",
    "if len(all_tables) == 0:\n",
    "    print(\"\\n⚠️  WARNING: No tables found in lakehouse!\")\n",
    "    print(\"Please run notebook 01_ingest_to_bronze.ipynb first to create Bronze tables.\")\n",
    "    bronze_tables = []\n",
    "    dimension_tables = []\n",
    "    fact_tables = []\n",
    "else:\n",
    "    # Filter for Bronze tables (exclude Silver_ and Gold_ prefixes)\n",
    "    bronze_tables = [t.name for t in all_tables \n",
    "                     if not t.name.lower().startswith(\"silver_\") \n",
    "                     and not t.name.lower().startswith(\"gold_\")]\n",
    "    \n",
    "    # Separate dimensions and facts\n",
    "    dimension_tables = [t for t in bronze_tables if t.startswith(\"dim\")]\n",
    "    fact_tables = [t for t in bronze_tables if t.startswith(\"fact\")]\n",
    "    \n",
    "    print(f\"\\nBronze tables discovered:\")\n",
    "    print(f\"  Dimensions: {len(dimension_tables)}\")\n",
    "    print(f\"  Facts: {len(fact_tables)}\")\n",
    "    \n",
    "    if dimension_tables:\n",
    "        print(f\"\\nDimension tables: {', '.join(sorted(dimension_tables))}\")\n",
    "    if fact_tables:\n",
    "        print(f\"\\nFact tables ({len(fact_tables)} total):\")\n",
    "        for i, t in enumerate(sorted(fact_tables), 1):\n",
    "            print(f\"  {i:2d}. {t}\")\n",
    "            if i >= 15:  # Show first 15\n",
    "                remaining = len(fact_tables) - 15\n",
    "                if remaining > 0:\n",
    "                    print(f\"  ... and {remaining} more\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf23819",
   "metadata": {},
   "source": [
    "## Transformation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155b2729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dim_customer(bronze_df):\n",
    "    \"\"\"Transform DimCustomer: standardize, clean, deduplicate.\"\"\"\n",
    "    \n",
    "    # Remove duplicates (keep most recent by ingestion timestamp)\n",
    "    window_spec = Window.partitionBy(\"customer_id\").orderBy(desc(\"_ingestion_timestamp\"))\n",
    "    df = bronze_df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "                  .filter(col(\"row_num\") == 1) \\\n",
    "                  .drop(\"row_num\")\n",
    "    \n",
    "    # Standardize text fields\n",
    "    df = df.withColumn(\"customer_name\", trim(col(\"customer_name\"))) \\\n",
    "           .withColumn(\"country\", upper(col(\"country\"))) \\\n",
    "           .withColumn(\"region\", upper(col(\"region\")))\n",
    "    \n",
    "    # Add derived fields\n",
    "    df = df.withColumn(\"customer_age_days\", \n",
    "                       datediff(current_date(), col(\"customer_since\")))\n",
    "    \n",
    "    # Filter out inactive customers with no history (optional)\n",
    "    # df = df.filter(col(\"is_active\") == True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def transform_fact_sales(bronze_df):\n",
    "    \"\"\"Transform FactSales: validate amounts, filter invalid records.\"\"\"\n",
    "    \n",
    "    # Remove records with negative amounts (data quality issue)\n",
    "    df = bronze_df.filter(col(\"net_amount\") >= 0) \\\n",
    "                  .filter(col(\"quantity\") > 0)\n",
    "    \n",
    "    # Standardize status values\n",
    "    df = df.withColumn(\"status\", initcap(col(\"status\")))\n",
    "    \n",
    "    # Add derived metrics\n",
    "    df = df.withColumn(\"margin_percent\", \n",
    "                       when(col(\"net_amount\") > 0, \n",
    "                            col(\"gross_margin\") / col(\"net_amount\") * 100)\n",
    "                       .otherwise(0))\n",
    "    \n",
    "    # Add date flags\n",
    "    df = df.withColumn(\"is_same_day_delivery\",\n",
    "                       col(\"order_date_id\") == col(\"delivery_date_id\"))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def transform_generic(bronze_df, table_name):\n",
    "    \"\"\"Generic transformation: remove duplicates, standardize.\"\"\"\n",
    "    \n",
    "    # Get primary key column (assume it's the first column with 'id' in name)\n",
    "    pk_cols = [c for c in bronze_df.columns if c.endswith(\"_id\")]\n",
    "    \n",
    "    if pk_cols:\n",
    "        pk_col = pk_cols[0]\n",
    "        # Remove duplicates\n",
    "        window_spec = Window.partitionBy(pk_col).orderBy(desc(\"_ingestion_timestamp\"))\n",
    "        df = bronze_df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "                      .filter(col(\"row_num\") == 1) \\\n",
    "                      .drop(\"row_num\")\n",
    "    else:\n",
    "        df = bronze_df\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f42e39c",
   "metadata": {},
   "source": [
    "## Transform Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edd586b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: Transforming Dimension Tables\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "dim_results = {}\n",
    "\n",
    "# Transform DimCustomer with custom logic\n",
    "if \"DimCustomer\" in dimension_tables:\n",
    "    try:\n",
    "        print(\"\\nTransforming DimCustomer...\")\n",
    "        bronze_customer = spark.table(\"dimcustomer\")\n",
    "        silver_customer = transform_dim_customer(bronze_customer)\n",
    "        \n",
    "        bronze_count = bronze_customer.count()\n",
    "        silver_count = silver_customer.count()\n",
    "        \n",
    "        silver_customer.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .saveAsTable(f\"{SILVER_PREFIX}DimCustomer\")\n",
    "        \n",
    "        print(f\"✅ Silver_DimCustomer created: {bronze_count:,} → {silver_count:,} rows\")\n",
    "        if bronze_count > silver_count:\n",
    "            print(f\"   Removed {bronze_count - silver_count:,} duplicate records\")\n",
    "        dim_results[\"DimCustomer\"] = True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error transforming DimCustomer: {str(e)}\")\n",
    "        dim_results[\"DimCustomer\"] = False\n",
    "\n",
    "# Transform all other dimensions with generic transformation\n",
    "other_dims = [t for t in dimension_tables if t != \"dimcustomer\"]\n",
    "\n",
    "for table in other_dims:\n",
    "    try:\n",
    "        print(f\"\\nTransforming {table}...\")\n",
    "        bronze_df = spark.table(table)\n",
    "        silver_df = transform_generic(bronze_df, table)\n",
    "        \n",
    "        bronze_count = bronze_df.count()\n",
    "        silver_count = silver_df.count()\n",
    "        \n",
    "        silver_df.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .saveAsTable(f\"{SILVER_PREFIX}{table}\")\n",
    "        \n",
    "        print(f\"✅ {SILVER_PREFIX}{table} created: {bronze_count:,} → {silver_count:,} rows\")\n",
    "        dim_results[table] = True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error transforming {table}: {str(e)}\")\n",
    "        dim_results[table] = False\n",
    "\n",
    "# Summary\n",
    "success_count = len([v for v in dim_results.values() if v])\n",
    "print(f\"\\n✅ Dimensions transformed: {success_count}/{len(dimension_tables)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385b51f1",
   "metadata": {},
   "source": [
    "## Transform Fact Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77bfc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: Transforming Fact Tables\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fact_results = {}\n",
    "\n",
    "# Transform FactSales with custom logic\n",
    "if \"FactSales\" in fact_tables:\n",
    "    try:\n",
    "        print(\"\\nTransforming FactSales...\")\n",
    "        bronze_sales = spark.table(\"factsales\")\n",
    "        silver_sales = transform_fact_sales(bronze_sales)\n",
    "        \n",
    "        bronze_count = bronze_sales.count()\n",
    "        silver_count = silver_sales.count()\n",
    "        \n",
    "        silver_sales.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .saveAsTable(f\"{SILVER_PREFIX}FactSales\")\n",
    "        \n",
    "        print(f\"✅ Silver_FactSales created: {bronze_count:,} → {silver_count:,} rows\")\n",
    "        if bronze_count > silver_count:\n",
    "            print(f\"   Filtered out {bronze_count - silver_count:,} invalid records\")\n",
    "        fact_results[\"FactSales\"] = True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error transforming FactSales: {str(e)}\")\n",
    "        fact_results[\"FactSales\"] = False\n",
    "\n",
    "# Transform all other fact tables with generic transformation\n",
    "other_facts = [t for t in fact_tables if t != \"factsales\"]\n",
    "\n",
    "for table in other_facts:\n",
    "    try:\n",
    "        print(f\"\\nTransforming {table}...\")\n",
    "        bronze_df = spark.table(table)\n",
    "        silver_df = transform_generic(bronze_df, table)\n",
    "        \n",
    "        silver_count = silver_df.count()\n",
    "        \n",
    "        silver_df.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .saveAsTable(f\"{SILVER_PREFIX}{table}\")\n",
    "        \n",
    "        print(f\"✅ {SILVER_PREFIX}{table} created: {silver_count:,} rows\")\n",
    "        fact_results[table] = True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error transforming {table}: {str(e)}\")\n",
    "        fact_results[table] = False\n",
    "\n",
    "# Summary\n",
    "success_count = len([v for v in fact_results.values() if v])\n",
    "print(f\"\\n✅ Fact tables transformed: {success_count}/{len(fact_tables)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6963f84b",
   "metadata": {},
   "source": [
    "## Data Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3895e56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: Data Quality Validation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Validate Silver_DimCustomer\n",
    "try:\n",
    "    df = spark.table(f\"{SILVER_PREFIX}DimCustomer\")\n",
    "    \n",
    "    # Check for nulls in key columns\n",
    "    null_checks = {\n",
    "        \"customer_id\": df.filter(col(\"customer_id\").isNull()).count(),\n",
    "        \"customer_name\": df.filter(col(\"customer_name\").isNull()).count()\n",
    "    }\n",
    "    \n",
    "    print(\"\\nSilver_DimCustomer Quality Checks:\")\n",
    "    for field, null_count in null_checks.items():\n",
    "        status = \"✅\" if null_count == 0 else \"⚠️\"\n",
    "        print(f\"  {status} {field}: {null_count} nulls\")\n",
    "    \n",
    "    # Check data consistency\n",
    "    distinct_count = df.select(\"customer_id\").distinct().count()\n",
    "    total_count = df.count()\n",
    "    if distinct_count == total_count:\n",
    "        print(f\"  ✅ Primary key uniqueness: OK ({distinct_count:,} unique customers)\")\n",
    "    else:\n",
    "        print(f\"  ⚠️  Duplicate customer_ids found: {total_count - distinct_count}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"⏭️  Skipping validation: {str(e)}\")\n",
    "\n",
    "# Validate Silver_FactSales\n",
    "try:\n",
    "    df = spark.table(f\"{SILVER_PREFIX}FactSales\")\n",
    "    \n",
    "    print(\"\\nSilver_FactSales Quality Checks:\")\n",
    "    \n",
    "    # Check for negative amounts\n",
    "    negative_count = df.filter(col(\"net_amount\") < 0).count()\n",
    "    status = \"✅\" if negative_count == 0 else \"⚠️\"\n",
    "    print(f\"  {status} Negative amounts: {negative_count}\")\n",
    "    \n",
    "    # Check for zero quantities\n",
    "    zero_qty = df.filter(col(\"quantity\") <= 0).count()\n",
    "    status = \"✅\" if zero_qty == 0 else \"⚠️\"\n",
    "    print(f\"  {status} Zero/negative quantities: {zero_qty}\")\n",
    "    \n",
    "    # Check status values\n",
    "    print(f\"\\n  Status distribution:\")\n",
    "    status_dist = df.groupBy(\"status\").count().orderBy(desc(\"count\"))\n",
    "    status_dist.show(10, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⏭️  Skipping validation: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a245d1c",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a54dd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all Silver tables\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SILVER TRANSFORMATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "silver_tables = [t for t in spark.catalog.listTables() if t.name.startswith(\"silver_\")]\n",
    "\n",
    "print(f\"\\nSilver tables created: {len(silver_tables)}\")\n",
    "print(f\"  Dimensions: {len([t for t in silver_tables if 'dim' in t.name.lower()])}\")\n",
    "print(f\"  Facts: {len([t for t in silver_tables if 'fact' in t.name.lower()])}\")\n",
    "\n",
    "print(\"\\nTable Details:\")\n",
    "for table in sorted(silver_tables, key=lambda x: x.name):\n",
    "    df = spark.table(table.name)\n",
    "    row_count = df.count()\n",
    "    col_count = len(df.columns)\n",
    "    table_type = \"DIM\" if \"dim\" in table.name.lower() else \"FACT\"\n",
    "    print(f\"  [{table_type:4s}] {table.name:35s} | {row_count:>10,} rows | {col_count:>3} cols\")\n",
    "\n",
    "print(f\"\\nCompletion Time: {datetime.now()}\")\n",
    "print(\"\\nNext Step: Run notebook 03_build_gold_star_schema.ipynb\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
