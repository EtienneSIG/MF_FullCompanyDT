{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33d9e753",
   "metadata": {},
   "source": [
    "# Silver Layer Transformation - Enterprise Data Platform\n",
    "\n",
    "## Overview\n",
    "This notebook transforms Bronze Delta tables into Silver layer with:\n",
    "- Data quality improvements\n",
    "- Data type standardization\n",
    "- Deduplication\n",
    "- Conformance to business rules\n",
    "\n",
    "**Prerequisites:**\n",
    "- Bronze Delta tables created (run 01_ingest_to_bronze.ipynb first)\n",
    "\n",
    "**Output:**\n",
    "- Silver Delta tables with cleaned, conformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aced21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"Silver Transformation Started: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f75a26",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deec9d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SILVER_PREFIX = \"Silver_\"\n",
    "\n",
    "# Tables to transform\n",
    "TABLES_TO_TRANSFORM = [\n",
    "    \"DimCustomer\",\n",
    "    \"DimProduct\", \n",
    "    \"DimEmployee\",\n",
    "    \"FactSales\",\n",
    "    \"FactSupport\",\n",
    "    \"FactAttrition\"\n",
    "]\n",
    "\n",
    "print(f\"Tables to transform: {len(TABLES_TO_TRANSFORM)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf23819",
   "metadata": {},
   "source": [
    "## Transformation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155b2729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dim_customer(bronze_df):\n",
    "    \"\"\"Transform DimCustomer: standardize, clean, deduplicate.\"\"\"\n",
    "    \n",
    "    # Remove duplicates (keep most recent by ingestion timestamp)\n",
    "    window_spec = Window.partitionBy(\"customer_id\").orderBy(desc(\"_ingestion_timestamp\"))\n",
    "    df = bronze_df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "                  .filter(col(\"row_num\") == 1) \\\n",
    "                  .drop(\"row_num\")\n",
    "    \n",
    "    # Standardize text fields\n",
    "    df = df.withColumn(\"customer_name\", trim(col(\"customer_name\"))) \\\n",
    "           .withColumn(\"country\", upper(col(\"country\"))) \\\n",
    "           .withColumn(\"region\", upper(col(\"region\")))\n",
    "    \n",
    "    # Add derived fields\n",
    "    df = df.withColumn(\"customer_age_days\", \n",
    "                       datediff(current_date(), col(\"customer_since\")))\n",
    "    \n",
    "    # Filter out inactive customers with no history (optional)\n",
    "    # df = df.filter(col(\"is_active\") == True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def transform_fact_sales(bronze_df):\n",
    "    \"\"\"Transform FactSales: validate amounts, filter invalid records.\"\"\"\n",
    "    \n",
    "    # Remove records with negative amounts (data quality issue)\n",
    "    df = bronze_df.filter(col(\"net_amount\") >= 0) \\\n",
    "                  .filter(col(\"quantity\") > 0)\n",
    "    \n",
    "    # Standardize status values\n",
    "    df = df.withColumn(\"status\", initcap(col(\"status\")))\n",
    "    \n",
    "    # Add derived metrics\n",
    "    df = df.withColumn(\"margin_percent\", \n",
    "                       when(col(\"net_amount\") > 0, \n",
    "                            col(\"gross_margin\") / col(\"net_amount\") * 100)\n",
    "                       .otherwise(0))\n",
    "    \n",
    "    # Add date flags\n",
    "    df = df.withColumn(\"is_same_day_delivery\",\n",
    "                       col(\"order_date_id\") == col(\"delivery_date_id\"))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def transform_generic(bronze_df, table_name):\n",
    "    \"\"\"Generic transformation: remove duplicates, standardize.\"\"\"\n",
    "    \n",
    "    # Get primary key column (assume it's the first column with 'id' in name)\n",
    "    pk_cols = [c for c in bronze_df.columns if c.endswith(\"_id\")]\n",
    "    \n",
    "    if pk_cols:\n",
    "        pk_col = pk_cols[0]\n",
    "        # Remove duplicates\n",
    "        window_spec = Window.partitionBy(pk_col).orderBy(desc(\"_ingestion_timestamp\"))\n",
    "        df = bronze_df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "                      .filter(col(\"row_num\") == 1) \\\n",
    "                      .drop(\"row_num\")\n",
    "    else:\n",
    "        df = bronze_df\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f42e39c",
   "metadata": {},
   "source": [
    "## Transform Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edd586b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: Transforming Dimension Tables\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Transform DimCustomer\n",
    "try:\n",
    "    print(\"\\nTransforming DimCustomer...\")\n",
    "    bronze_customer = spark.table(\"DimCustomer\")\n",
    "    silver_customer = transform_dim_customer(bronze_customer)\n",
    "    \n",
    "    bronze_count = bronze_customer.count()\n",
    "    silver_count = silver_customer.count()\n",
    "    \n",
    "    silver_customer.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(f\"{SILVER_PREFIX}DimCustomer\")\n",
    "    \n",
    "    print(f\"✅ Silver_DimCustomer created: {bronze_count:,} → {silver_count:,} rows\")\n",
    "    if bronze_count > silver_count:\n",
    "        print(f\"   Removed {bronze_count - silver_count:,} duplicate records\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error transforming DimCustomer: {str(e)}\")\n",
    "\n",
    "# Transform other dimensions with generic transformation\n",
    "for table in [\"DimProduct\", \"DimEmployee\"]:\n",
    "    try:\n",
    "        print(f\"\\nTransforming {table}...\")\n",
    "        bronze_df = spark.table(table)\n",
    "        silver_df = transform_generic(bronze_df, table)\n",
    "        \n",
    "        bronze_count = bronze_df.count()\n",
    "        silver_count = silver_df.count()\n",
    "        \n",
    "        silver_df.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .saveAsTable(f\"{SILVER_PREFIX}{table}\")\n",
    "        \n",
    "        print(f\"✅ {SILVER_PREFIX}{table} created: {bronze_count:,} → {silver_count:,} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"⏭️  Skipping {table}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385b51f1",
   "metadata": {},
   "source": [
    "## Transform Fact Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77bfc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: Transforming Fact Tables\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Transform FactSales\n",
    "try:\n",
    "    print(\"\\nTransforming FactSales...\")\n",
    "    bronze_sales = spark.table(\"FactSales\")\n",
    "    silver_sales = transform_fact_sales(bronze_sales)\n",
    "    \n",
    "    bronze_count = bronze_sales.count()\n",
    "    silver_count = silver_sales.count()\n",
    "    \n",
    "    silver_sales.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(f\"{SILVER_PREFIX}FactSales\")\n",
    "    \n",
    "    print(f\"✅ Silver_FactSales created: {bronze_count:,} → {silver_count:,} rows\")\n",
    "    if bronze_count > silver_count:\n",
    "        print(f\"   Filtered out {bronze_count - silver_count:,} invalid records\")\n",
    "except Exception as e:\n",
    "    print(f\"⏭️  Skipping FactSales: {str(e)}\")\n",
    "\n",
    "# Transform other fact tables with generic transformation\n",
    "for table in [\"FactSupport\", \"FactAttrition\", \"FactInventory\"]:\n",
    "    try:\n",
    "        print(f\"\\nTransforming {table}...\")\n",
    "        bronze_df = spark.table(table)\n",
    "        silver_df = transform_generic(bronze_df, table)\n",
    "        \n",
    "        silver_count = silver_df.count()\n",
    "        \n",
    "        silver_df.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .saveAsTable(f\"{SILVER_PREFIX}{table}\")\n",
    "        \n",
    "        print(f\"✅ {SILVER_PREFIX}{table} created: {silver_count:,} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"⏭️  Skipping {table}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6963f84b",
   "metadata": {},
   "source": [
    "## Data Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3895e56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: Data Quality Validation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Validate Silver_DimCustomer\n",
    "try:\n",
    "    df = spark.table(f\"{SILVER_PREFIX}DimCustomer\")\n",
    "    \n",
    "    # Check for nulls in key columns\n",
    "    null_checks = {\n",
    "        \"customer_id\": df.filter(col(\"customer_id\").isNull()).count(),\n",
    "        \"customer_name\": df.filter(col(\"customer_name\").isNull()).count()\n",
    "    }\n",
    "    \n",
    "    print(\"\\nSilver_DimCustomer Quality Checks:\")\n",
    "    for field, null_count in null_checks.items():\n",
    "        status = \"✅\" if null_count == 0 else \"⚠️\"\n",
    "        print(f\"  {status} {field}: {null_count} nulls\")\n",
    "    \n",
    "    # Check data consistency\n",
    "    distinct_count = df.select(\"customer_id\").distinct().count()\n",
    "    total_count = df.count()\n",
    "    if distinct_count == total_count:\n",
    "        print(f\"  ✅ Primary key uniqueness: OK ({distinct_count:,} unique customers)\")\n",
    "    else:\n",
    "        print(f\"  ⚠️  Duplicate customer_ids found: {total_count - distinct_count}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"⏭️  Skipping validation: {str(e)}\")\n",
    "\n",
    "# Validate Silver_FactSales\n",
    "try:\n",
    "    df = spark.table(f\"{SILVER_PREFIX}FactSales\")\n",
    "    \n",
    "    print(\"\\nSilver_FactSales Quality Checks:\")\n",
    "    \n",
    "    # Check for negative amounts\n",
    "    negative_count = df.filter(col(\"net_amount\") < 0).count()\n",
    "    status = \"✅\" if negative_count == 0 else \"⚠️\"\n",
    "    print(f\"  {status} Negative amounts: {negative_count}\")\n",
    "    \n",
    "    # Check for zero quantities\n",
    "    zero_qty = df.filter(col(\"quantity\") <= 0).count()\n",
    "    status = \"✅\" if zero_qty == 0 else \"⚠️\"\n",
    "    print(f\"  {status} Zero/negative quantities: {zero_qty}\")\n",
    "    \n",
    "    # Check status values\n",
    "    print(f\"\\n  Status distribution:\")\n",
    "    status_dist = df.groupBy(\"status\").count().orderBy(desc(\"count\"))\n",
    "    status_dist.show(10, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⏭️  Skipping validation: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a245d1c",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a54dd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all Silver tables\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SILVER TRANSFORMATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "silver_tables = [t for t in spark.catalog.listTables() if t.name.startswith(\"silver_\")]\n",
    "\n",
    "print(f\"\\nSilver tables created: {len(silver_tables)}\\n\")\n",
    "\n",
    "for table in sorted(silver_tables, key=lambda x: x.name):\n",
    "    df = spark.table(table.name)\n",
    "    row_count = df.count()\n",
    "    col_count = len(df.columns)\n",
    "    print(f\"  {table.name:35s} | {row_count:>10,} rows | {col_count:>3} columns\")\n",
    "\n",
    "print(f\"\\nCompletion Time: {datetime.now()}\")\n",
    "print(\"\\nNext Step: Run notebook 03_build_gold_star_schema.ipynb\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
