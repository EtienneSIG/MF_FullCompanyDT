{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6978ab21",
   "metadata": {},
   "source": [
    "# Data Quality Checks - Enterprise Data Platform\n",
    "\n",
    "## Overview\n",
    "This notebook performs comprehensive data quality checks on Gold layer tables.\n",
    "\n",
    "**Checks Performed:**\n",
    "- Referential integrity (FK validation)\n",
    "- Business rule compliance\n",
    "- Data distribution analysis\n",
    "- Anomaly detection\n",
    "\n",
    "**Prerequisites:**\n",
    "- Gold star schema created (run 03_build_gold_star_schema.ipynb first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65367f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "print(f\"Data Quality Checks Started: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a7954c",
   "metadata": {},
   "source": [
    "## Check 1: Referential Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c21a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECK 1: Referential Integrity Validation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check if lakehouse is attached\n",
    "try:\n",
    "    current_db = spark.catalog.currentDatabase()\n",
    "    print(f\"\\n‚úÖ Using database/lakehouse: {current_db}\")\n",
    "except Exception as e:\n",
    "    print(\"\\n‚ùå ERROR: No lakehouse attached!\")\n",
    "    print(\"Please attach a lakehouse to this notebook before running.\")\n",
    "    print(\"Steps:\")\n",
    "    print(\"  1. Click on the lakehouse icon in the left panel\")\n",
    "    print(\"  2. Select 'Add' and choose your lakehouse\")\n",
    "    print(\"  3. Re-run this cell\")\n",
    "    raise Exception(\"No lakehouse attached. Please attach a lakehouse and try again.\") from None\n",
    "\n",
    "# Get all Gold fact and dimension tables dynamically\n",
    "try:\n",
    "    gold_facts = [t.name for t in spark.catalog.listTables() \n",
    "                  if t.name.startswith(\"gold_fact\")]\n",
    "    gold_dims = [t.name for t in spark.catalog.listTables() \n",
    "                 if t.name.startswith(\"gold_dim\") or t.name.startswith(\"dim\")]\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERROR: Cannot list tables: {str(e)}\")\n",
    "    print(\"Make sure you have attached a lakehouse with Gold layer tables.\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\nüìä Found {len(gold_facts)} fact tables and {len(gold_dims)} dimension tables to validate\")\n",
    "\n",
    "if len(gold_facts) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: No Gold fact tables found!\")\n",
    "    print(\"Please run notebook 03_build_gold_star_schema.ipynb first to create Gold tables.\")\n",
    "    integrity_results = []\n",
    "else:\n",
    "    integrity_results = []\n",
    "    total_checks = 0\n",
    "    passed_checks = 0\n",
    "    failed_checks = 0\n",
    "\n",
    "    # For each fact table, verify all foreign key relationships\n",
    "    for fact_table_name in sorted(gold_facts):\n",
    "        try:\n",
    "            fact_df = spark.table(fact_table_name)\n",
    "            fact_columns = fact_df.columns\n",
    "            \n",
    "            print(f\"\\nüîç Validating {fact_table_name}...\")\n",
    "            \n",
    "            # Find all potential foreign key columns (ending with _id or _key)\n",
    "            fk_columns = [col for col in fact_columns \n",
    "                         if col.endswith('_id') or col.endswith('_key')]\n",
    "            \n",
    "            if not fk_columns:\n",
    "                print(f\"   ‚ÑπÔ∏è  No foreign key columns found\")\n",
    "                continue\n",
    "            \n",
    "            # For each foreign key column, try to find matching dimension table\n",
    "            for fk_col in fk_columns:\n",
    "                # Try to infer dimension table name\n",
    "                possible_dim_names = []\n",
    "                \n",
    "                # Extract base name from FK column\n",
    "                if fk_col.endswith('_date_id'):\n",
    "                    possible_dim_names = ['dimdate', 'gold_dimdate']\n",
    "                elif fk_col.endswith('_id'):\n",
    "                    base_name = fk_col.replace('_id', '')\n",
    "                    possible_dim_names = [\n",
    "                        f'gold_dim{base_name}',\n",
    "                        f'dim{base_name}',\n",
    "                        f'gold_dim{base_name.replace(\"_\", \"\")}',\n",
    "                        f'dim{base_name.replace(\"_\", \"\")}'\n",
    "                    ]\n",
    "                elif fk_col.endswith('_key'):\n",
    "                    base_name = fk_col.replace('_key', '')\n",
    "                    possible_dim_names = [\n",
    "                        f'gold_dim{base_name}',\n",
    "                        f'dim{base_name}',\n",
    "                        f'gold_dim{base_name.replace(\"_\", \"\")}',\n",
    "                        f'dim{base_name.replace(\"_\", \"\")}'\n",
    "                    ]\n",
    "                \n",
    "                # Find matching dimension table\n",
    "                matching_dim = None\n",
    "                for dim_name in possible_dim_names:\n",
    "                    if dim_name in [d.lower() for d in gold_dims]:\n",
    "                        matching_dim = [d for d in gold_dims if d.lower() == dim_name][0]\n",
    "                        break\n",
    "                \n",
    "                if not matching_dim:\n",
    "                    # Check if the FK might reference a dimension with different naming\n",
    "                    for dim_table in gold_dims:\n",
    "                        dim_df = spark.table(dim_table)\n",
    "                        if fk_col in dim_df.columns:\n",
    "                            matching_dim = dim_table\n",
    "                            break\n",
    "                \n",
    "                if matching_dim:\n",
    "                    total_checks += 1\n",
    "                    try:\n",
    "                        # Get dimension table\n",
    "                        dim_df = spark.table(matching_dim)\n",
    "                        \n",
    "                        # Find the primary key column in dimension\n",
    "                        pk_candidates = [fk_col]\n",
    "                        if '_' in fk_col:\n",
    "                            # For order_date_id, try date_id\n",
    "                            parts = fk_col.split('_')\n",
    "                            if len(parts) > 2:\n",
    "                                pk_candidates.append('_'.join(parts[-2:]))\n",
    "                        \n",
    "                        pk_col = None\n",
    "                        for candidate in pk_candidates:\n",
    "                            if candidate in dim_df.columns:\n",
    "                                pk_col = candidate\n",
    "                                break\n",
    "                        \n",
    "                        if not pk_col:\n",
    "                            print(f\"   ‚ö†Ô∏è  {fk_col} ‚Üí {matching_dim}: Cannot find PK column\")\n",
    "                            failed_checks += 1\n",
    "                            continue\n",
    "                        \n",
    "                        # Find orphaned FKs (excluding nulls)\n",
    "                        orphaned = fact_df.select(fk_col).distinct() \\\n",
    "                            .join(dim_df.select(pk_col), \n",
    "                                  fact_df[fk_col] == dim_df[pk_col], \n",
    "                                  \"left_anti\") \\\n",
    "                            .filter(col(fk_col).isNotNull())\n",
    "                        \n",
    "                        orphan_count = orphaned.count()\n",
    "                        total_distinct = fact_df.select(fk_col).filter(col(fk_col).isNotNull()).distinct().count()\n",
    "                        \n",
    "                        status = \"‚úÖ PASS\" if orphan_count == 0 else \"‚ùå FAIL\"\n",
    "                        \n",
    "                        print(f\"   {status} {fk_col} ‚Üí {matching_dim}.{pk_col}\")\n",
    "                        print(f\"        Orphaned: {orphan_count:,} / {total_distinct:,} distinct values\")\n",
    "                        \n",
    "                        if orphan_count == 0:\n",
    "                            passed_checks += 1\n",
    "                        else:\n",
    "                            failed_checks += 1\n",
    "                            # Show sample orphaned values\n",
    "                            if orphan_count <= 5:\n",
    "                                print(f\"        Orphaned values:\")\n",
    "                                orphaned.show(orphan_count, truncate=False)\n",
    "                            else:\n",
    "                                print(f\"        Sample orphaned values:\")\n",
    "                                orphaned.show(5, truncate=False)\n",
    "                        \n",
    "                        integrity_results.append({\n",
    "                            \"fact_table\": fact_table_name,\n",
    "                            \"fk_column\": fk_col,\n",
    "                            \"dim_table\": matching_dim,\n",
    "                            \"pk_column\": pk_col,\n",
    "                            \"orphan_count\": orphan_count,\n",
    "                            \"total_distinct\": total_distinct,\n",
    "                            \"passed\": orphan_count == 0\n",
    "                        })\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        print(f\"   ‚ùå Error checking {fk_col} ‚Üí {matching_dim}: {str(e)}\")\n",
    "                        failed_checks += 1\n",
    "                else:\n",
    "                    # FK column doesn't match any dimension table (might be a measure or non-FK column)\n",
    "                    print(f\"   ‚ÑπÔ∏è  {fk_col}: No matching dimension table found (might not be a FK)\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {fact_table_name}: {str(e)}\")\n",
    "\n",
    "    # Summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"REFERENTIAL INTEGRITY VALIDATION SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total relationship checks: {total_checks}\")\n",
    "    print(f\"‚úÖ Passed: {passed_checks}\")\n",
    "    print(f\"‚ùå Failed: {failed_checks}\")\n",
    "\n",
    "    if failed_checks == 0 and total_checks > 0:\n",
    "        print(\"\\nüéâ All referential integrity checks passed!\")\n",
    "    elif total_checks == 0:\n",
    "        print(\"\\n‚ö†Ô∏è  No relationships could be validated\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  {failed_checks} relationship(s) have orphaned records\")\n",
    "        print(\"   Review the failed checks above and consider cleaning data or updating dimension tables\")\n",
    "\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00918709",
   "metadata": {},
   "source": [
    "## Check 2: Null Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd4aa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECK 2: Null Value Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get all Gold tables dynamically\n",
    "all_gold_tables = spark.catalog.listTables()\n",
    "tables_to_check = [t.name for t in all_gold_tables \n",
    "                   if t.name.startswith(\"gold_dim\") or t.name.startswith(\"gold_fact\") ]\n",
    "\n",
    "if not tables_to_check:\n",
    "    print(\"\\n‚ö†Ô∏è  No Gold tables found to analyze\")\n",
    "else:\n",
    "    print(f\"\\nüìä Analyzing {len(tables_to_check)} tables for null values\\n\")\n",
    "    \n",
    "    for table_name in sorted(tables_to_check):\n",
    "        try:\n",
    "            print(f\"\\n{table_name}:\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            df = spark.table(table_name)\n",
    "            total_rows = df.count()\n",
    "            \n",
    "            # Calculate null percentage for each column\n",
    "            null_stats = []\n",
    "            for col_name in df.columns:\n",
    "                null_count = df.filter(col(col_name).isNull()).count()\n",
    "                null_pct = (null_count / total_rows * 100) if total_rows > 0 else 0\n",
    "                \n",
    "                if null_count > 0:\n",
    "                    null_stats.append({\n",
    "                        \"column\": col_name,\n",
    "                        \"null_count\": null_count,\n",
    "                        \"null_percentage\": null_pct\n",
    "                    })\n",
    "            \n",
    "            if null_stats:\n",
    "                print(f\"  {'Column':<30s} | {'Null Count':>12} | {'Percentage':>10}\")\n",
    "                print(\"  \" + \"-\" * 76)\n",
    "                for stat in sorted(null_stats, key=lambda x: x[\"null_percentage\"], reverse=True):\n",
    "                    status = \"‚ö†Ô∏è\" if stat[\"null_percentage\"] > 10 else \"‚ÑπÔ∏è\"\n",
    "                    print(f\"  {status} {stat['column']:28s} | {stat['null_count']:>12,} | {stat['null_percentage']:>9.2f}%\")\n",
    "            else:\n",
    "                print(\"  ‚úÖ No null values found in any column\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚è≠Ô∏è  Skipping: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc79a62a",
   "metadata": {},
   "source": [
    "## Check 3: Business Rule Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd18c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECK 3: Business Rule Validation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get all Gold fact tables\n",
    "gold_facts = [t.name for t in spark.catalog.listTables() \n",
    "              if t.name.startswith(\"gold_fact\")]\n",
    "\n",
    "if not gold_facts:\n",
    "    print(\"\\n‚ö†Ô∏è  No Gold fact tables found for business rule validation\")\n",
    "else:\n",
    "    print(f\"\\nüìä Validating business rules for {len(gold_facts)} fact table(s)\\n\")\n",
    "    \n",
    "    business_rule_violations = 0\n",
    "    business_rule_checks = 0\n",
    "    \n",
    "    for fact_table in sorted(gold_facts):\n",
    "        try:\n",
    "            df = spark.table(fact_table)\n",
    "            columns = df.columns\n",
    "            total_rows = df.count()\n",
    "            \n",
    "            print(f\"\\nüîç Validating {fact_table}:\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            table_has_rules = False\n",
    "            \n",
    "            # Rule 1: Numeric amount/value columns should be non-negative\n",
    "            amount_cols = [c for c in columns \n",
    "                          if any(keyword in c.lower() for keyword in ['amount', 'value', 'price', 'cost', 'revenue', 'total'])\n",
    "                          and df.schema[c].dataType.simpleString() in ['double', 'decimal', 'float', 'int', 'bigint']]\n",
    "            \n",
    "            if amount_cols:\n",
    "                table_has_rules = True\n",
    "                for amt_col in amount_cols:\n",
    "                    try:\n",
    "                        business_rule_checks += 1\n",
    "                        negative_amounts = df.filter(col(amt_col) < 0).count()\n",
    "                        status = \"‚úÖ PASS\" if negative_amounts == 0 else \"‚ùå FAIL\"\n",
    "                        \n",
    "                        print(f\"  {status} Non-negative values in '{amt_col}'\")\n",
    "                        print(f\"       Violations: {negative_amounts:,} / {total_rows:,} rows\")\n",
    "                        \n",
    "                        if negative_amounts > 0:\n",
    "                            business_rule_violations += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"  ‚ö†Ô∏è  Error checking {amt_col}: {str(e)}\")\n",
    "            \n",
    "            # Rule 2: Quantity columns should be positive\n",
    "            qty_cols = [c for c in columns \n",
    "                       if 'quantity' in c.lower() or 'qty' in c.lower()\n",
    "                       and df.schema[c].dataType.simpleString() in ['double', 'decimal', 'float', 'int', 'bigint']]\n",
    "            \n",
    "            if qty_cols:\n",
    "                table_has_rules = True\n",
    "                for qty_col in qty_cols:\n",
    "                    try:\n",
    "                        business_rule_checks += 1\n",
    "                        invalid_qty = df.filter(col(qty_col) <= 0).count()\n",
    "                        status = \"‚úÖ PASS\" if invalid_qty == 0 else \"‚ùå FAIL\"\n",
    "                        \n",
    "                        print(f\"  {status} Positive values in '{qty_col}'\")\n",
    "                        print(f\"       Violations: {invalid_qty:,} / {total_rows:,} rows\")\n",
    "                        \n",
    "                        if invalid_qty > 0:\n",
    "                            business_rule_violations += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"  ‚ö†Ô∏è  Error checking {qty_col}: {str(e)}\")\n",
    "            \n",
    "            # Rule 3: Date logic - ship_date >= order_date, end_date >= start_date, etc.\n",
    "            date_pairs = []\n",
    "            \n",
    "            # Check for common date pair patterns\n",
    "            if 'ship_date_id' in columns and 'order_date_id' in columns:\n",
    "                date_pairs.append(('ship_date_id', 'order_date_id', 'Ship date should be >= Order date'))\n",
    "            if 'delivery_date_id' in columns and 'ship_date_id' in columns:\n",
    "                date_pairs.append(('delivery_date_id', 'ship_date_id', 'Delivery date should be >= Ship date'))\n",
    "            if 'end_date_id' in columns and 'start_date_id' in columns:\n",
    "                date_pairs.append(('end_date_id', 'start_date_id', 'End date should be >= Start date'))\n",
    "            if 'close_date_id' in columns and 'create_date_id' in columns:\n",
    "                date_pairs.append(('close_date_id', 'create_date_id', 'Close date should be >= Create date'))\n",
    "            \n",
    "            if date_pairs:\n",
    "                table_has_rules = True\n",
    "                for later_date, earlier_date, rule_desc in date_pairs:\n",
    "                    try:\n",
    "                        business_rule_checks += 1\n",
    "                        invalid_dates = df.filter(\n",
    "                            (col(later_date).isNotNull()) & \n",
    "                            (col(earlier_date).isNotNull()) & \n",
    "                            (col(later_date) < col(earlier_date))\n",
    "                        ).count()\n",
    "                        \n",
    "                        status = \"‚úÖ PASS\" if invalid_dates == 0 else \"‚ùå FAIL\"\n",
    "                        \n",
    "                        print(f\"  {status} {rule_desc}\")\n",
    "                        print(f\"       Violations: {invalid_dates:,} / {total_rows:,} rows\")\n",
    "                        \n",
    "                        if invalid_dates > 0:\n",
    "                            business_rule_violations += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"  ‚ö†Ô∏è  Error checking date rule: {str(e)}\")\n",
    "            \n",
    "            if not table_has_rules:\n",
    "                print(f\"  ‚ÑπÔ∏è  No standard business rules applicable to this table\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error processing {fact_table}: {str(e)}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BUSINESS RULE VALIDATION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total business rule checks: {business_rule_checks}\")\n",
    "    print(f\"‚úÖ Passed: {business_rule_checks - business_rule_violations}\")\n",
    "    print(f\"‚ùå Failed: {business_rule_violations}\")\n",
    "    \n",
    "    if business_rule_violations == 0 and business_rule_checks > 0:\n",
    "        print(\"\\nüéâ All business rules validated successfully!\")\n",
    "    elif business_rule_checks == 0:\n",
    "        print(\"\\n‚ö†Ô∏è  No business rules were validated\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  {business_rule_violations} business rule violation(s) found\")\n",
    "    \n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f5bd58",
   "metadata": {},
   "source": [
    "## Check 4: Data Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b0ab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECK 4: Data Distribution Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get all Gold fact tables\n",
    "gold_facts = [t.name for t in spark.catalog.listTables() \n",
    "              if t.name.startswith(\"gold_fact\")]\n",
    "\n",
    "if not gold_facts:\n",
    "    print(\"\\n‚ö†Ô∏è  No Gold fact tables found for distribution analysis\")\n",
    "else:\n",
    "    print(f\"\\nüìä Analyzing data distribution for {len(gold_facts)} fact table(s)\\n\")\n",
    "    \n",
    "    # Analyze each fact table\n",
    "    for fact_table in sorted(gold_facts):\n",
    "        try:\n",
    "            print(f\"\\n{fact_table} - Record Count by Date:\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            df = spark.table(fact_table)\n",
    "            \n",
    "            # Find date columns\n",
    "            date_cols = [c for c in df.columns if c.endswith('_date_id')]\n",
    "            \n",
    "            if date_cols:\n",
    "                # Use first date column found\n",
    "                date_col = date_cols[0]\n",
    "                \n",
    "                # Show distribution by year-month\n",
    "                monthly_dist = df \\\n",
    "                    .withColumn(\"year_month\", substring(col(date_col).cast(\"string\"), 1, 6)) \\\n",
    "                    .groupBy(\"year_month\") \\\n",
    "                    .agg(count(\"*\").alias(\"record_count\")) \\\n",
    "                    .orderBy(\"year_month\")\n",
    "                \n",
    "                print(f\"  Based on column: {date_col}\")\n",
    "                monthly_dist.show(12, truncate=False)\n",
    "            else:\n",
    "                # Just show total count\n",
    "                total = df.count()\n",
    "                print(f\"  Total records: {total:,}\")\n",
    "                print(f\"  (No date columns found for temporal distribution)\")\n",
    "            \n",
    "            # If there's a status column, show distribution by status\n",
    "            if \"status\" in df.columns:\n",
    "                print(f\"\\n{fact_table} - Distribution by Status:\")\n",
    "                print(\"-\" * 80)\n",
    "                \n",
    "                status_dist = df.groupBy(\"status\") \\\n",
    "                    .agg(count(\"*\").alias(\"count\")) \\\n",
    "                    .withColumn(\"percentage\", \n",
    "                                round(col(\"count\") / df.count() * 100, 2)) \\\n",
    "                    .orderBy(desc(\"count\"))\n",
    "                \n",
    "                status_dist.show(truncate=False)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚è≠Ô∏è  Skipping {fact_table}: {str(e)}\")\n",
    "\n",
    "# Analyze dimension tables for categorical distributions\n",
    "gold_dims = [t.name for t in spark.catalog.listTables() \n",
    "             if t.name.startswith(\"gold_dim\") or \n",
    "                (t.name.startswith(\"dim\") and not t.name.startswith(\"gold_\"))]\n",
    "\n",
    "            print(f\"  ‚è≠Ô∏è  Skipping {dim_table}: {str(e)}\")\n",
    "\n",
    "if gold_dims:        except Exception as e:\n",
    "\n",
    "    print(f\"\\n\\nüìä Analyzing categorical distributions for {len(gold_dims)} dimension table(s)\\n\")            \n",
    "\n",
    "                    dist.show(10, truncate=False)\n",
    "\n",
    "    for dim_table in sorted(gold_dims)[:3]:  # Show first 3 dimensions to avoid too much output                \n",
    "\n",
    "        try:                    .orderBy(desc(\"count\"))\n",
    "\n",
    "            df = spark.table(dim_table)                                round(col(\"count\") / df.count() * 100, 2)) \\\n",
    "\n",
    "                                .withColumn(\"percentage\", \n",
    "\n",
    "            # Find categorical columns (non-ID, non-date columns)                    .agg(count(\"*\").alias(\"count\")) \\\n",
    "\n",
    "            categorical_cols = [c for c in df.columns                 dist = df.groupBy(col_to_analyze) \\\n",
    "\n",
    "                               if not c.endswith('_id') and not c.endswith('_key')                 \n",
    "\n",
    "                               and not 'date' in c.lower()                 print(\"-\" * 80)\n",
    "\n",
    "                               and df.schema[c].dataType.simpleString() == 'string']                print(f\"\\n{dim_table} - Distribution by {col_to_analyze}:\")\n",
    "\n",
    "                            \n",
    "\n",
    "            if categorical_cols:                col_to_analyze = categorical_cols[0]\n",
    "                # Show distribution for first categorical column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6cee68",
   "metadata": {},
   "source": [
    "## Check 5: Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05629959",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECK 5: Anomaly Detection\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get all Gold fact tables\n",
    "gold_facts = [t.name for t in spark.catalog.listTables() \n",
    "              if t.name.startswith(\"gold_fact\")]\n",
    "\n",
    "if not gold_facts:\n",
    "    print(\"\\n‚ö†Ô∏è  No Gold fact tables found for anomaly detection\")\n",
    "else:\n",
    "    print(f\"\\nüìä Detecting outliers in {len(gold_facts)} fact table(s) (>3œÉ from mean)\\n\")\n",
    "    \n",
    "    # Analyze each fact table\n",
    "    for fact_table in sorted(gold_facts):\n",
    "        try:\n",
    "            df = spark.table(fact_table)\n",
    "            \n",
    "            # Find numeric amount/value columns\n",
    "            numeric_cols = [c for c in df.columns \n",
    "                           if any(keyword in c.lower() for keyword in ['amount', 'value', 'price', 'cost', 'revenue'])\n",
    "                           and df.schema[c].dataType.simpleString() in ['double', 'decimal', 'float', 'int', 'bigint']]\n",
    "            \n",
    "            if not numeric_cols:\n",
    "                print(f\"\\n{fact_table}:\")\n",
    "                print(f\"  ‚ÑπÔ∏è  No numeric amount columns found for outlier detection\\n\")\n",
    "                continue\n",
    "            \n",
    "            # Analyze first numeric column found\n",
    "            amount_col = numeric_cols[0]\n",
    "            \n",
    "            print(f\"\\n{fact_table} - Outlier Detection on '{amount_col}':\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            # Calculate statistics\n",
    "            stats = df.select(\n",
    "                mean(amount_col).alias(\"mean_amount\"),\n",
    "                stddev(amount_col).alias(\"stddev_amount\"),\n",
    "                min(amount_col).alias(\"min_amount\"),\n",
    "\n",
    "                max(amount_col).alias(\"max_amount\")            print(f\"  ‚è≠Ô∏è  Skipping: {str(e)}\\n\")\n",
    "\n",
    "            ).collect()[0]            print(f\"\\n{fact_table}:\")\n",
    "\n",
    "                    except Exception as e:\n",
    "\n",
    "            mean_val = stats[\"mean_amount\"]            \n",
    "\n",
    "            stddev_val = stats[\"stddev_amount\"]                print(f\"  ‚úÖ No outliers detected\\n\")\n",
    "\n",
    "            min_val = stats[\"min_amount\"]            else:\n",
    "\n",
    "            max_val = stats[\"max_amount\"]                outliers.select(*cols_to_show).show(5, truncate=False)\n",
    "\n",
    "                            \n",
    "\n",
    "            if mean_val is None or stddev_val is None:                    cols_to_show = df.columns[:5]  # Show first 5 columns\n",
    "\n",
    "                print(f\"  ‚ö†Ô∏è  Cannot calculate statistics (null values)\\n\")                if not cols_to_show:\n",
    "\n",
    "                continue                cols_to_show = [c for c in df.columns if c in ['order_id', 'transaction_id', 'id'] + [amount_col]]\n",
    "\n",
    "                            # Show key columns if they exist\n",
    "\n",
    "            threshold = mean_val + (3 * stddev_val)                print(f\"\\n  Top 5 outliers:\")\n",
    "\n",
    "                        if outlier_count > 0:\n",
    "\n",
    "            # Find outliers            \n",
    "\n",
    "            outliers = df.filter(col(amount_col) > threshold) \\            print(f\"  Outliers found: {outlier_count:,} / {total_count:,} records ({outlier_count/total_count*100:.2f}%)\")\n",
    "\n",
    "                .orderBy(desc(amount_col))            print(f\"  Outlier threshold (>3œÉ): ${threshold:,.2f}\")\n",
    "\n",
    "                        print(f\"  Range: ${min_val:,.2f} to ${max_val:,.2f}\")\n",
    "\n",
    "            outlier_count = outliers.count()            print(f\"  Std dev: ${stddev_val:,.2f}\")\n",
    "\n",
    "            total_count = df.count()            print(f\"  Mean: ${mean_val:,.2f}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236d2e0a",
   "metadata": {},
   "source": [
    "## Quality Report Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa04ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA QUALITY REPORT - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compile overall score\n",
    "checks_passed = 0\n",
    "total_checks = 0\n",
    "\n",
    "# Count integrity checks\n",
    "if integrity_results:\n",
    "    total_checks += len(integrity_results)\n",
    "    checks_passed += sum(1 for r in integrity_results if r[\"passed\"])\n",
    "\n",
    "print(f\"\\n‚úÖ Checks Passed: {checks_passed}\")\n",
    "print(f\"‚ö†Ô∏è  Checks Failed: {total_checks - checks_passed}\")\n",
    "print(f\"üìä Total Checks: {total_checks}\")\n",
    "\n",
    "if total_checks > 0:\n",
    "    quality_score = (checks_passed / total_checks) * 100\n",
    "    print(f\"\\nüéØ Data Quality Score: {quality_score:.1f}%\")\n",
    "    \n",
    "    if quality_score >= 90:\n",
    "        print(\"   ‚úÖ EXCELLENT - Data is production-ready\")\n",
    "    elif quality_score >= 75:\n",
    "        print(\"   ‚ö†Ô∏è  GOOD - Minor issues to address\")\n",
    "    else:\n",
    "        print(\"   ‚ùå NEEDS IMPROVEMENT - Review failed checks\")\n",
    "\n",
    "print(f\"\\nCompletion Time: {datetime.now()}\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
