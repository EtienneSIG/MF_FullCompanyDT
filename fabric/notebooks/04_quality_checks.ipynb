{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6978ab21",
   "metadata": {},
   "source": [
    "# Data Quality Checks - Enterprise Data Platform\n",
    "\n",
    "## Overview\n",
    "This notebook performs comprehensive data quality checks on Gold layer tables.\n",
    "\n",
    "**Checks Performed:**\n",
    "- Referential integrity (FK validation)\n",
    "- Business rule compliance\n",
    "- Data distribution analysis\n",
    "- Anomaly detection\n",
    "\n",
    "**Prerequisites:**\n",
    "- Gold star schema created (run 03_build_gold_star_schema.ipynb first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65367f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "print(f\"Data Quality Checks Started: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a7954c",
   "metadata": {},
   "source": [
    "## Check 1: Referential Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c21a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECK 1: Referential Integrity Validation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define FK relationships to check\n",
    "relationships = [\n",
    "    (\"FactSales\", \"customer_id\", \"DimCustomer\", \"customer_id\"),\n",
    "    (\"FactSales\", \"product_id\", \"DimProduct\", \"product_id\"),\n",
    "    (\"FactSales\", \"employee_id\", \"DimEmployee\", \"employee_id\"),\n",
    "    (\"FactSales\", \"order_date_id\", \"DimDate\", \"date_id\"),\n",
    "]\n",
    "\n",
    "integrity_results = []\n",
    "\n",
    "for fact_table, fk_col, dim_table, pk_col in relationships:\n",
    "    try:\n",
    "        fact = spark.table(fact_table)\n",
    "        dim = spark.table(dim_table)\n",
    "        \n",
    "        # Find orphaned FKs\n",
    "        orphaned = fact.select(fk_col).distinct() \\\n",
    "            .join(dim.select(pk_col), fact[fk_col] == dim[pk_col], \"left_anti\") \\\n",
    "            .filter(col(fk_col).isNotNull())\n",
    "        \n",
    "        orphan_count = orphaned.count()\n",
    "        total_distinct = fact.select(fk_col).filter(col(fk_col).isNotNull()).distinct().count()\n",
    "        \n",
    "        status = \"‚úÖ PASS\" if orphan_count == 0 else \"‚ùå FAIL\"\n",
    "        \n",
    "        print(f\"\\n{status} {fact_table}.{fk_col} ‚Üí {dim_table}.{pk_col}\")\n",
    "        print(f\"   Orphaned: {orphan_count:,} / {total_distinct:,} distinct values\")\n",
    "        \n",
    "        integrity_results.append({\n",
    "            \"relationship\": f\"{fact_table}.{fk_col} ‚Üí {dim_table}.{pk_col}\",\n",
    "            \"orphan_count\": orphan_count,\n",
    "            \"passed\": orphan_count == 0\n",
    "        })\n",
    "        \n",
    "        if orphan_count > 0:\n",
    "            print(\"   Sample orphaned values:\")\n",
    "            orphaned.show(5, truncate=False)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚è≠Ô∏è  Skipping {fact_table} ‚Üí {dim_table}: {str(e)}\")\n",
    "\n",
    "# Summary\n",
    "passed = sum(1 for r in integrity_results if r[\"passed\"])\n",
    "total = len(integrity_results)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Referential Integrity: {passed}/{total} checks passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00918709",
   "metadata": {},
   "source": [
    "## Check 2: Null Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd4aa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECK 2: Null Value Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Tables to check\n",
    "tables_to_check = [\"DimCustomer\", \"DimProduct\", \"FactSales\"]\n",
    "\n",
    "for table_name in tables_to_check:\n",
    "    try:\n",
    "        print(f\"\\n{table_name}:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        df = spark.table(table_name)\n",
    "        total_rows = df.count()\n",
    "        \n",
    "        # Calculate null percentage for each column\n",
    "        null_stats = []\n",
    "        for col_name in df.columns:\n",
    "            null_count = df.filter(col(col_name).isNull()).count()\n",
    "            null_pct = (null_count / total_rows * 100) if total_rows > 0 else 0\n",
    "            \n",
    "            if null_count > 0:\n",
    "                null_stats.append({\n",
    "                    \"column\": col_name,\n",
    "                    \"null_count\": null_count,\n",
    "                    \"null_percentage\": null_pct\n",
    "                })\n",
    "        \n",
    "        if null_stats:\n",
    "            for stat in sorted(null_stats, key=lambda x: x[\"null_percentage\"], reverse=True):\n",
    "                status = \"‚ö†Ô∏è\" if stat[\"null_percentage\"] > 10 else \"‚ÑπÔ∏è\"\n",
    "                print(f\"  {status} {stat['column']:30s} | {stat['null_count']:>8,} nulls ({stat['null_percentage']:>5.2f}%)\")\n",
    "        else:\n",
    "            print(\"  ‚úÖ No null values found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚è≠Ô∏è  Skipping: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc79a62a",
   "metadata": {},
   "source": [
    "## Check 3: Business Rule Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd18c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECK 3: Business Rule Validation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Rule 1: FactSales - amounts should be non-negative\n",
    "try:\n",
    "    fact_sales = spark.table(\"FactSales\")\n",
    "    \n",
    "    negative_amounts = fact_sales.filter(\n",
    "        (col(\"net_amount\") < 0) | \n",
    "        (col(\"total_amount\") < 0) | \n",
    "        (col(\"quantity\") <= 0)\n",
    "    ).count()\n",
    "    \n",
    "    total_rows = fact_sales.count()\n",
    "    status = \"‚úÖ PASS\" if negative_amounts == 0 else \"‚ùå FAIL\"\n",
    "    \n",
    "    print(f\"\\n{status} FactSales: Non-negative amounts rule\")\n",
    "    print(f\"   Violations: {negative_amounts:,} / {total_rows:,} rows\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚è≠Ô∏è  Skipping FactSales amount check: {str(e)}\")\n",
    "\n",
    "# Rule 2: FactSales - ship_date >= order_date\n",
    "try:\n",
    "    fact_sales = spark.table(\"FactSales\")\n",
    "    \n",
    "    invalid_dates = fact_sales.filter(\n",
    "        col(\"ship_date_id\") < col(\"order_date_id\")\n",
    "    ).count()\n",
    "    \n",
    "    total_rows = fact_sales.count()\n",
    "    status = \"‚úÖ PASS\" if invalid_dates == 0 else \"‚ùå FAIL\"\n",
    "    \n",
    "    print(f\"\\n{status} FactSales: Ship date >= Order date rule\")\n",
    "    print(f\"   Violations: {invalid_dates:,} / {total_rows:,} rows\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚è≠Ô∏è  Skipping FactSales date check: {str(e)}\")\n",
    "\n",
    "# Rule 3: DimCustomer - credit_limit should be positive\n",
    "try:\n",
    "    dim_customer = spark.table(\"DimCustomer\")\n",
    "    \n",
    "    invalid_credit = dim_customer.filter(col(\"credit_limit\") <= 0).count()\n",
    "    total_rows = dim_customer.count()\n",
    "    status = \"‚úÖ PASS\" if invalid_credit == 0 else \"‚ùå FAIL\"\n",
    "    \n",
    "    print(f\"\\n{status} DimCustomer: Positive credit limit rule\")\n",
    "    print(f\"   Violations: {invalid_credit:,} / {total_rows:,} rows\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚è≠Ô∏è  Skipping DimCustomer credit check: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f5bd58",
   "metadata": {},
   "source": [
    "## Check 4: Data Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b0ab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECK 4: Data Distribution Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# FactSales - Revenue by status\n",
    "try:\n",
    "    print(\"\\nFactSales - Revenue Distribution by Status:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    fact_sales = spark.table(\"FactSales\")\n",
    "    \n",
    "    status_dist = fact_sales.groupBy(\"status\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"order_count\"),\n",
    "            sum(\"net_amount\").alias(\"total_revenue\"),\n",
    "            avg(\"net_amount\").alias(\"avg_order_value\")\n",
    "        ) \\\n",
    "        .orderBy(desc(\"total_revenue\"))\n",
    "    \n",
    "    status_dist.show(10, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚è≠Ô∏è  Skipping: {str(e)}\")\n",
    "\n",
    "# DimCustomer - Distribution by segment\n",
    "try:\n",
    "    print(\"\\nDimCustomer - Distribution by Segment:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    dim_customer = spark.table(\"DimCustomer\")\n",
    "    \n",
    "    segment_dist = dim_customer.groupBy(\"segment\") \\\n",
    "        .agg(count(\"*\").alias(\"customer_count\")) \\\n",
    "        .withColumn(\"percentage\", \n",
    "                    round(col(\"customer_count\") / dim_customer.count() * 100, 2)) \\\n",
    "        .orderBy(desc(\"customer_count\"))\n",
    "    \n",
    "    segment_dist.show(truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚è≠Ô∏è  Skipping: {str(e)}\")\n",
    "\n",
    "# FactSales - Monthly revenue trend\n",
    "try:\n",
    "    print(\"\\nFactSales - Monthly Revenue Trend:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    fact_sales = spark.table(\"FactSales\")\n",
    "    \n",
    "    monthly_revenue = fact_sales \\\n",
    "        .withColumn(\"year_month\", substring(col(\"order_date_id\").cast(\"string\"), 1, 6)) \\\n",
    "        .filter(col(\"status\").isin([\"Delivered\", \"Shipped\"])) \\\n",
    "        .groupBy(\"year_month\") \\\n",
    "        .agg(\n",
    "            sum(\"net_amount\").alias(\"revenue\"),\n",
    "            count(\"*\").alias(\"order_count\")\n",
    "        ) \\\n",
    "        .orderBy(\"year_month\")\n",
    "    \n",
    "    monthly_revenue.show(12, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚è≠Ô∏è  Skipping: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6cee68",
   "metadata": {},
   "source": [
    "## Check 5: Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05629959",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECK 5: Anomaly Detection\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Detect unusually high order amounts (>3 std dev from mean)\n",
    "try:\n",
    "    print(\"\\nFactSales - Outlier Detection (Unusually High Order Amounts):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    fact_sales = spark.table(\"FactSales\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats = fact_sales.select(\n",
    "        mean(\"net_amount\").alias(\"mean_amount\"),\n",
    "        stddev(\"net_amount\").alias(\"stddev_amount\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    mean_val = stats[\"mean_amount\"]\n",
    "    stddev_val = stats[\"stddev_amount\"]\n",
    "    threshold = mean_val + (3 * stddev_val)\n",
    "    \n",
    "    outliers = fact_sales.filter(col(\"net_amount\") > threshold) \\\n",
    "        .select(\"order_id\", \"customer_id\", \"net_amount\", \"quantity\", \"status\") \\\n",
    "        .orderBy(desc(\"net_amount\"))\n",
    "    \n",
    "    outlier_count = outliers.count()\n",
    "    total_count = fact_sales.count()\n",
    "    \n",
    "    print(f\"   Mean order amount: ${mean_val:,.2f}\")\n",
    "    print(f\"   Std dev: ${stddev_val:,.2f}\")\n",
    "    print(f\"   Outlier threshold (>3œÉ): ${threshold:,.2f}\")\n",
    "    print(f\"   Outliers found: {outlier_count:,} / {total_count:,} orders ({outlier_count/total_count*100:.2f}%)\")\n",
    "    \n",
    "    if outlier_count > 0:\n",
    "        print(f\"\\n   Top 10 outliers:\")\n",
    "        outliers.show(10, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚è≠Ô∏è  Skipping: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236d2e0a",
   "metadata": {},
   "source": [
    "## Quality Report Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa04ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA QUALITY REPORT - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compile overall score\n",
    "checks_passed = 0\n",
    "total_checks = 0\n",
    "\n",
    "# Count integrity checks\n",
    "if integrity_results:\n",
    "    total_checks += len(integrity_results)\n",
    "    checks_passed += sum(1 for r in integrity_results if r[\"passed\"])\n",
    "\n",
    "print(f\"\\n‚úÖ Checks Passed: {checks_passed}\")\n",
    "print(f\"‚ö†Ô∏è  Checks Failed: {total_checks - checks_passed}\")\n",
    "print(f\"üìä Total Checks: {total_checks}\")\n",
    "\n",
    "if total_checks > 0:\n",
    "    quality_score = (checks_passed / total_checks) * 100\n",
    "    print(f\"\\nüéØ Data Quality Score: {quality_score:.1f}%\")\n",
    "    \n",
    "    if quality_score >= 90:\n",
    "        print(\"   ‚úÖ EXCELLENT - Data is production-ready\")\n",
    "    elif quality_score >= 75:\n",
    "        print(\"   ‚ö†Ô∏è  GOOD - Minor issues to address\")\n",
    "    else:\n",
    "        print(\"   ‚ùå NEEDS IMPROVEMENT - Review failed checks\")\n",
    "\n",
    "print(f\"\\nCompletion Time: {datetime.now()}\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
